{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATAcq7bQyNxe"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.883 · Aprendizaje por refuerzo</p>\n",
    "<p style=\"margin: 0; text-align:right;\">Máster universitario en Ciencia de datos (<i>Data science</i>)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Informática, Multimedia y Telecomunicación</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>\n",
    "\n",
    "\n",
    "# PEC2: Deep Reinforcement Learning\n",
    " \n",
    "\n",
    "En esta práctica se implementarán tres modelos de DRL en un mismo entorno, con el objetivo de analizar distintas formas de aprendizaje de un agente y estudiar su rendimiento. El agente será entrenado con los métodos:\n",
    "\n",
    "<ol>\n",
    "    <li>DQN</li>\n",
    "    <li>Double DQN (DDQN)</li>\n",
    "    <li>REINFORCE con línea de base</li>\n",
    "    <li>REINFORCE con reward modificado </li>\n",
    " </ol>\n",
    " \n",
    "Finalmente, crearemos un entorno de cero y lo resolveremos.\n",
    " \n",
    " \n",
    "**Importante: La entrega debe hacerse en formato notebook y en formato html donde se vea el código y los resultados y comentarios de cada ejercicio. Para exportar el notebook a html puede hacerse desde el menú File  →  Download as  →  HTML.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZXQSvJSyNxk"
   },
   "source": [
    "## 0. Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zan-tCv_zJXF"
   },
   "source": [
    "**Mountain Car** consiste en un coche situado en un valle. El coche tiene muy poca potencia y sólo con su motor no es capaz de salir del valle porque la gravedad es más fuerte. Para ello, el agente debe conseguir impulsar el coche, ya sea acelerando hacia adelante o hacia atrás a la velocidad más adecuada para poder  llegar a la cima de la montaña y salir del valle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JMd-EGKzJXF"
   },
   "source": [
    "Las acciones que puede realizar el coche son las siguientes:\n",
    "<ul>\n",
    "    <li>0 : Acelerar hacia a la izquierda</li>\n",
    "    <li>1 : No usar el motor</li>\n",
    "    <li>2 : Acelerar hacia la derecha</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](mountainCar.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9Z1HGctzJXG"
   },
   "source": [
    "El agente recibirá una recompensa (penalización) de -1 cada vez que no consiga llegar a la posición objetivo.\n",
    "\n",
    "Para más detalles sobre la definición del entorno de MountainCar, se recomienda consultar el código fuente: https://github.com/openai/gym/blob/master/gym/envs/classic_control/mountain_car.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUsJh2LMzJXG"
   },
   "source": [
    "Cada episodio termina cuando el coche ha llegado al punto objetivo o si se han realizado 200 pasos. La solución óptima es aquella en la que el agente, con un desplazamiento eficiente, consigue llegar a la cima en el menor número de pasos posible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMMw53RSzJXH"
   },
   "source": [
    "<!-- Sure, in the second solution, I modified the reward function so that the agent received a reward based on its position. If the agent failed an episode by not getting to the top of the hill but got further to the right from the previous episode it still learns that going right is better even though it ultimately failed the task. Without this reinforcement, the agent has to reach the top of the hill by random chance before receiving a reward signal that causes it to modify its behavior in future episodes. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTgKXBHJyNxo"
   },
   "source": [
    "## 1. Inicialización y exploración del entorno (0.5 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8FuAhvNyNxp"
   },
   "source": [
    "Empezaremos cargando las principales librerías necesarias para la práctica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "arbFPOZ4yNxp"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLoPqSUfyNxs"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.1 ptos):</strong> Inicializar el entorno 'MountainCar-v0'. Extraer:\n",
    "    \n",
    "   <ul>\n",
    "        <li>Valor del umbral de recompensa definido en el entorno</li>\n",
    "        <li>Máximo de pasos establecidos por cada episodio</li>\n",
    "        <li>Dimensión del espacio de acciones</li>\n",
    "        <li>Espacio de estados</li>\n",
    "        <li>Dimensión del espacio de estados</li>\n",
    "        <li>Valores mínimos permitidos de posición y velocidad</li>\n",
    "       <li>Valores máximos permitidos de posición y velocidad</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor del umbral de recompensa definido en el entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-110.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.spec.reward_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máximo de pasos establecidos por cada episodio:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensión del espacio de acciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Espacio de estados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-1.2  -0.07], [0.6  0.07], (2,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensión del espacio de estados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor mínimo de posición:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.min_position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor mínimo de velocidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valor máximo de posición y velocidad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.max_speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4XpV-t2yNxy"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.2 ptos):</strong> Mostrar la representación vectorial de cada observación del entorno junto con la acción aleatoria seleccionada (ej. 'Observation: [..] , Action: Accelerate left'), en un intervalo de 10 episodios de 100 pasos cada uno.\n",
    "    \n",
    "<i>Opcional</i>: visualizar los 10 episodios <code>env.render()</code>, sólo posible en local\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.55671525  0.        ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5574673  -0.00075203]\n",
      "Action: Accelerate to the Left\n",
      "[-0.55896574 -0.00149846]\n",
      "Action: Don't accelerate\n",
      "[-0.56019944 -0.0012337 ]\n",
      "Action: Accelerate to the Right\n",
      "[-5.6015921e-01  4.0251074e-05]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5608453 -0.0006861]\n",
      "Action: Don't accelerate\n",
      "[-5.6125265e-01 -4.0732717e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56037813  0.00087448]\n",
      "Action: Accelerate to the Left\n",
      "[-5.6022841e-01  1.4976243e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.5980444e-01  4.2393195e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5581095   0.00169494]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5571562   0.00095331]\n",
      "Action: Don't accelerate\n",
      "[-0.55595165  0.00120457]\n",
      "Action: Don't accelerate\n",
      "[-0.5545048   0.00144683]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55182654  0.00267829]\n",
      "Action: Don't accelerate\n",
      "[-0.5489368   0.00288975]\n",
      "Action: Don't accelerate\n",
      "[-0.5458572  0.0030796]\n",
      "Action: Don't accelerate\n",
      "[-0.54261076  0.00324642]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53822184  0.00438893]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5347233   0.00349857]\n",
      "Action: Don't accelerate\n",
      "[-0.5311413   0.00358198]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5265027   0.00463855]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5228424   0.00366033]\n",
      "Action: Don't accelerate\n",
      "[-0.51918775  0.00365465]\n",
      "Action: Don't accelerate\n",
      "[-0.5155662   0.00362157]\n",
      "Action: Don't accelerate\n",
      "[-0.51200485  0.00356133]\n",
      "Action: Don't accelerate\n",
      "[-0.50853044  0.0034744 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50416905  0.00436142]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50095326  0.00321578]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49890718  0.00204607]\n",
      "Action: Don't accelerate\n",
      "[-0.4970461   0.00186105]\n",
      "Action: Accelerate to the Right\n",
      "[-0.494384    0.00266212]\n",
      "Action: Don't accelerate\n",
      "[-0.4919407   0.00244329]\n",
      "Action: Don't accelerate\n",
      "[-0.4897345   0.00220621]\n",
      "Action: Don't accelerate\n",
      "[-0.48778182  0.00195266]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4850973   0.00268455]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48370084  0.00139643]\n",
      "Action: Accelerate to the Left\n",
      "[-4.8360294e-01  9.7913689e-05]\n",
      "Action: Don't accelerate\n",
      "[-4.8380426e-01 -2.0133588e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48330337  0.00050091]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4821039   0.00119943]\n",
      "Action: Accelerate to the Left\n",
      "[-4.8221490e-01 -1.1097458e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48363546 -0.00142056]\n",
      "Action: Accelerate to the Left\n",
      "[-0.486355   -0.00271956]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48835334 -0.00199831]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4896155  -0.00126216]\n",
      "Action: Don't accelerate\n",
      "[-0.49113208 -0.00151659]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4918918  -0.00075971]\n",
      "Action: Don't accelerate\n",
      "[-0.49288896 -0.00099715]\n",
      "Action: Don't accelerate\n",
      "[-0.4941161  -0.00122715]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4965641  -0.00244798]\n",
      "Action: Don't accelerate\n",
      "[-0.4992146  -0.00265052]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5030478  -0.00383324]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5060351  -0.00298727]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50815403 -0.00211894]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5113888  -0.00323473]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51571506 -0.00432629]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52110046 -0.00538541]\n",
      "Action: Don't accelerate\n",
      "[-0.52650464 -0.00540415]\n",
      "Action: Accelerate to the Left\n",
      "[-0.532887   -0.00638235]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53819966 -0.0053127 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5444029  -0.00620323]\n",
      "Action: Don't accelerate\n",
      "[-0.5504502 -0.0060473]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55529636 -0.00484613]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5589051  -0.00360876]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5632496  -0.00434446]\n",
      "Action: Don't accelerate\n",
      "[-0.56729734 -0.00404778]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5700183  -0.00272097]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5733923  -0.00337395]\n",
      "Action: Accelerate to the Right\n",
      "[-0.57539415 -0.00200189]\n",
      "Action: Don't accelerate\n",
      "[-0.57700914 -0.00161498]\n",
      "Action: Don't accelerate\n",
      "[-0.57822526 -0.00121612]\n",
      "Action: Accelerate to the Right\n",
      "[-5.7803351e-01  1.9174916e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.7843530e-01 -4.0180254e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.57942766 -0.00099238]\n",
      "Action: Accelerate to the Right\n",
      "[-5.7900333e-01  4.2438085e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.7916528e-01 -1.6199583e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.57791245  0.00125283]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5772541   0.00065838]\n",
      "Action: Accelerate to the Left\n",
      "[-5.7719505e-01  5.9057718e-05]\n",
      "Action: Accelerate to the Right\n",
      "[-0.57573575  0.0014593 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.57488704  0.00084873]\n",
      "Action: Don't accelerate\n",
      "[-0.5736551   0.00123188]\n",
      "Action: Accelerate to the Left\n",
      "[-0.57304925  0.00060589]\n",
      "Action: Don't accelerate\n",
      "[-0.5720738   0.00097541]\n",
      "Action: Accelerate to the Left\n",
      "[-5.717361e-01  3.376960e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.7203865e-01 -3.0252745e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5709792   0.00105949]\n",
      "Action: Don't accelerate\n",
      "[-0.56956553  0.00141365]\n",
      "Action: Don't accelerate\n",
      "[-0.5678082   0.00175731]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5647203   0.00308791]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5603247   0.00439554]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5546543   0.00567043]\n",
      "Action: Don't accelerate\n",
      "[-0.5487513   0.00590301]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54365987  0.00509147]\n",
      "Action: Accelerate to the Right\n",
      "[-0.537418    0.00624184]\n",
      "Action: Don't accelerate\n",
      "[-0.53107256  0.00634545]\n",
      "Action: Don't accelerate\n",
      "[-0.524671   0.0064015]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5192615   0.00540954]\n",
      "Action: Don't accelerate\n",
      "[-0.5138845   0.00537702]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5329652  0.       ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53389496 -0.00092976]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5357475  -0.00185255]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53850895 -0.00276146]\n",
      "Action: Don't accelerate\n",
      "[-0.5411586  -0.00264967]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5426767  -0.00151803]\n",
      "Action: Accelerate to the Right\n",
      "[-5.4305166e-01 -3.7502340e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5422809   0.00077079]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54037005  0.00191083]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5373335   0.00303656]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53319395  0.00413955]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5279825  0.0052115]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52173805  0.00624438]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5165076   0.00523042]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5123304   0.00417724]\n",
      "Action: Don't accelerate\n",
      "[-0.50823766  0.00409275]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5052601   0.00297758]\n",
      "Action: Don't accelerate\n",
      "[-0.50242     0.00284011]\n",
      "Action: Don't accelerate\n",
      "[-0.4997386   0.00268137]\n",
      "Action: Don't accelerate\n",
      "[-0.49723604  0.00250257]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49593097  0.00130506]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49383318  0.00209779]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49095833  0.00287484]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4873279   0.00363043]\n",
      "Action: Accelerate to the Right\n",
      "[-0.482969    0.00435894]\n",
      "Action: Don't accelerate\n",
      "[-0.478914    0.00405497]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47419316  0.00472083]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46884152  0.00535165]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4648987   0.00394281]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46039388  0.00450483]\n",
      "Action: Don't accelerate\n",
      "[-0.45636025  0.00403363]\n",
      "Action: Don't accelerate\n",
      "[-0.45282748  0.00353276]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45082152  0.00200595]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44835708  0.00246445]\n",
      "Action: Don't accelerate\n",
      "[-0.44645217  0.00190492]\n",
      "Action: Accelerate to the Left\n",
      "[-4.4612071e-01  3.3146716e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.4636512e-01 -2.4440023e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4481836  -0.00181848]\n",
      "Action: Don't accelerate\n",
      "[-0.45056286 -0.00237928]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45448557 -0.00392268]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45992288 -0.00543732]\n",
      "Action: Don't accelerate\n",
      "[-0.4658349  -0.00591199]\n",
      "Action: Don't accelerate\n",
      "[-0.47217792 -0.00634306]\n",
      "Action: Don't accelerate\n",
      "[-0.4789051  -0.00672719]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48696652 -0.00806139]\n",
      "Action: Don't accelerate\n",
      "[-0.49530208 -0.00833558]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5048496  -0.00954755]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5135377  -0.00868809]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52330124 -0.00976354]\n",
      "Action: Accelerate to the Right\n",
      "[-0.532067   -0.00876577]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54176927 -0.00970226]\n",
      "Action: Don't accelerate\n",
      "[-0.55133533 -0.00956605]\n",
      "Action: Don't accelerate\n",
      "[-0.5606936  -0.00935827]\n",
      "Action: Accelerate to the Left\n",
      "[-0.57077426 -0.01008063]\n",
      "Action: Don't accelerate\n",
      "[-0.5805022  -0.00972799]\n",
      "Action: Don't accelerate\n",
      "[-0.58980554 -0.00930329]\n",
      "Action: Accelerate to the Right\n",
      "[-0.59761554 -0.00781   ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.60587496 -0.00825942]\n",
      "Action: Accelerate to the Left\n",
      "[-0.61452353 -0.0086486 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.62149864 -0.00697509]\n",
      "Action: Don't accelerate\n",
      "[-0.62775    -0.00625135]\n",
      "Action: Don't accelerate\n",
      "[-0.63323283 -0.00548286]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6369082  -0.00367535]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63875   -0.0018418]\n",
      "Action: Accelerate to the Right\n",
      "[-6.3874525e-01  4.7558415e-06]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6368939   0.00185128]\n",
      "Action: Don't accelerate\n",
      "[-0.6342092   0.00268473]\n",
      "Action: Don't accelerate\n",
      "[-0.63071007  0.00349917]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6274213   0.00328875]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6243664   0.00305489]\n",
      "Action: Accelerate to the Left\n",
      "[-0.62156725  0.00279919]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6170438   0.00452342]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6108287   0.00621511]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6049668   0.00586189]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5975007   0.00746611]\n",
      "Action: Don't accelerate\n",
      "[-0.5894849   0.00801585]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5819781   0.00750678]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5750357   0.00694239]\n",
      "Action: Don't accelerate\n",
      "[-0.567709    0.00732664]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56105256  0.0066565 ]\n",
      "Action: Don't accelerate\n",
      "[-0.5541157   0.00693681]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5479503   0.00616537]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5406025   0.00734785]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5341272   0.00647532]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52657294  0.00755427]\n",
      "Action: Don't accelerate\n",
      "[-0.51899636  0.00757657]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5124543   0.00654206]\n",
      "Action: Don't accelerate\n",
      "[-0.5059958   0.00645849]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5006693   0.00532653]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49451458  0.00615469]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48957774  0.00493684]\n",
      "Action: Don't accelerate\n",
      "[-0.48489562  0.00468212]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47950312  0.0053925 ]\n",
      "Action: Don't accelerate\n",
      "[-0.47444037  0.00506275]\n",
      "Action: Accelerate to the Right\n",
      "[-0.468745    0.00569539]\n",
      "Action: Accelerate to the Left\n",
      "[-0.46445915  0.00428584]\n",
      "Action: Don't accelerate\n",
      "[-0.46061453  0.00384462]\n",
      "Action: Don't accelerate\n",
      "[-0.45723948  0.00337504]\n",
      "Action: Accelerate to the Right\n",
      "[-0.45335883  0.00388063]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44900113  0.00435772]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54260427  0.        ]\n",
      "Action: Don't accelerate\n",
      "[-5.4246181e-01  1.4246388e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.4217792e-01  2.8386098e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54275477 -0.00057687]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5441881  -0.00143328]\n",
      "Action: Accelerate to the Right\n",
      "[-5.4446703e-01 -2.7895486e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5435896   0.00087745]\n",
      "Action: Don't accelerate\n",
      "[-0.5425623  0.0010273]\n",
      "Action: Accelerate to the Left\n",
      "[-5.4239285e-01  1.6944550e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5410825   0.00131033]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5386411   0.00244139]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5350869   0.00355417]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53044665  0.00464032]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52475494  0.00569167]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5200546   0.00470034]\n",
      "Action: Don't accelerate\n",
      "[-0.51538086  0.00467376]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5097687   0.00561213]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5052603   0.00450844]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5018893   0.00337097]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49768102  0.00420826]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49466696  0.00301408]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4928696   0.00179736]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49030238  0.00256722]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48898447  0.00131791]\n",
      "Action: Don't accelerate\n",
      "[-0.4879257   0.00105877]\n",
      "Action: Don't accelerate\n",
      "[-0.48713398  0.00079173]\n",
      "Action: Accelerate to the Left\n",
      "[-4.8761517e-01 -4.8120983e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-4.8736575e-01  2.4943706e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48638752  0.00097822]\n",
      "Action: Accelerate to the Left\n",
      "[-4.866878e-01 -3.002810e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48826435 -0.00157655]\n",
      "Action: Don't accelerate\n",
      "[-0.49010542 -0.00184106]\n",
      "Action: Don't accelerate\n",
      "[-0.49219725 -0.00209184]\n",
      "Action: Don't accelerate\n",
      "[-0.49452424 -0.002327  ]\n",
      "Action: Don't accelerate\n",
      "[-0.49706903 -0.00254478]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4988126  -0.00174355]\n",
      "Action: Don't accelerate\n",
      "[-0.50074184 -0.00192927]\n",
      "Action: Don't accelerate\n",
      "[-0.5028424  -0.00210056]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50409853 -0.00125614]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50650084 -0.00240231]\n",
      "Action: Don't accelerate\n",
      "[-0.50903136 -0.00253048]\n",
      "Action: Accelerate to the Right\n",
      "[-0.510671  -0.0016397]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5114077  -0.00073664]\n",
      "Action: Don't accelerate\n",
      "[-0.51223576 -0.00082805]\n",
      "Action: Accelerate to the Left\n",
      "[-0.514149   -0.00191326]\n",
      "Action: Don't accelerate\n",
      "[-0.5161331  -0.00198412]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51717323 -0.00104011]\n",
      "Action: Don't accelerate\n",
      "[-0.5182615 -0.0010883]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52038985 -0.00212833]\n",
      "Action: Don't accelerate\n",
      "[-0.52254224 -0.00215239]\n",
      "Action: Don't accelerate\n",
      "[-0.52470255 -0.00216032]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52785456 -0.00315204]\n",
      "Action: Don't accelerate\n",
      "[-0.5309747  -0.00312012]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5330395  -0.00206481]\n",
      "Action: Don't accelerate\n",
      "[-0.5350335  -0.00199401]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5359418  -0.00090827]\n",
      "Action: Accelerate to the Right\n",
      "[-5.3575748e-01  1.8428628e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53648204 -0.00072454]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53810996 -0.00162794]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54062915 -0.00251914]\n",
      "Action: Don't accelerate\n",
      "[-0.5430206  -0.00239147]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5462665  -0.00324589]\n",
      "Action: Don't accelerate\n",
      "[-0.5493425  -0.00307601]\n",
      "Action: Accelerate to the Left\n",
      "[-0.55322564 -0.00388313]\n",
      "Action: Don't accelerate\n",
      "[-0.55688685 -0.00366122]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5592988  -0.00241197]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56244355 -0.00314473]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5642976  -0.00185406]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5668472  -0.00254957]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56807333 -0.00122612]\n",
      "Action: Don't accelerate\n",
      "[-0.56896687 -0.00089355]\n",
      "Action: Accelerate to the Right\n",
      "[-5.685212e-01  4.456619e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.6873965e-01 -2.1843893e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5676206   0.00111908]\n",
      "Action: Accelerate to the Left\n",
      "[-5.671722e-01  4.482880e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.673981e-01 -2.258408e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5682964  -0.00089829]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56986046 -0.00156406]\n",
      "Action: Don't accelerate\n",
      "[-0.57107866 -0.00121821]\n",
      "Action: Don't accelerate\n",
      "[-0.571942   -0.00086332]\n",
      "Action: Don't accelerate\n",
      "[-5.7244396e-01 -5.0201156e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.725810e-01 -1.369821e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.7235193e-01  2.2906371e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.7275850e-01 -4.0659003e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5717977   0.00096077]\n",
      "Action: Don't accelerate\n",
      "[-0.5704767   0.00132101]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56980526  0.00067143]\n",
      "Action: Don't accelerate\n",
      "[-0.5687884   0.00101687]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56643367  0.00235476]\n",
      "Action: Don't accelerate\n",
      "[-0.5637585   0.00267514]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5617829   0.00197561]\n",
      "Action: Don't accelerate\n",
      "[-0.55952156  0.00226136]\n",
      "Action: Accelerate to the Left\n",
      "[-0.55799127  0.00153026]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55520356  0.00278775]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5511791   0.00402443]\n",
      "Action: Don't accelerate\n",
      "[-0.5469481   0.00423105]\n",
      "Action: Don't accelerate\n",
      "[-0.54254204  0.00440602]\n",
      "Action: Accelerate to the Left\n",
      "[-0.538994    0.00354802]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5343306   0.00466344]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53058666  0.00374392]\n",
      "Action: Don't accelerate\n",
      "[-0.4497255  0.       ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44927505  0.00045047]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44837737  0.00089765]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4470391   0.00133827]\n",
      "Action: Accelerate to the Left\n",
      "[-4.472700e-01 -2.308927e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.44906837 -0.00179837]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45242107 -0.0033527 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.45530355 -0.00288249]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45969468 -0.00439112]\n",
      "Action: Don't accelerate\n",
      "[-0.46456215 -0.00486747]\n",
      "Action: Don't accelerate\n",
      "[-0.4698701  -0.00530794]\n",
      "Action: Accelerate to the Left\n",
      "[-0.47657925 -0.00670916]\n",
      "Action: Don't accelerate\n",
      "[-0.4836399  -0.00706064]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4919995  -0.00835961]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49959576 -0.00759625]\n",
      "Action: Don't accelerate\n",
      "[-0.5073719  -0.00777612]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5162696  -0.00889777]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5262224  -0.00995274]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5351555  -0.00893306]\n",
      "Action: Don't accelerate\n",
      "[-0.5440019 -0.0088464]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55169535 -0.00769348]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56017834 -0.008483  ]\n",
      "Action: Don't accelerate\n",
      "[-0.5683875 -0.0082092]\n",
      "Action: Don't accelerate\n",
      "[-0.5762618 -0.0078743]\n",
      "Action: Don't accelerate\n",
      "[-0.5837428  -0.00748097]\n",
      "Action: Accelerate to the Right\n",
      "[-0.58977515 -0.00603233]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5963144  -0.00653926]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6033126  -0.00699821]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6107186  -0.00740604]\n",
      "Action: Don't accelerate\n",
      "[-0.61747867 -0.00676005]\n",
      "Action: Accelerate to the Right\n",
      "[-0.62254393 -0.00506522]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6278779  -0.00533399]\n",
      "Action: Accelerate to the Left\n",
      "[-0.63344246 -0.00556459]\n",
      "Action: Accelerate to the Left\n",
      "[-0.63919806 -0.00575559]\n",
      "Action: Don't accelerate\n",
      "[-0.64410394 -0.00490587]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6491256  -0.00502164]\n",
      "Action: Don't accelerate\n",
      "[-0.65322787 -0.00410228]\n",
      "Action: Accelerate to the Left\n",
      "[-0.65738225 -0.00415438]\n",
      "Action: Accelerate to the Right\n",
      "[-0.65955997 -0.00217773]\n",
      "Action: Don't accelerate\n",
      "[-0.66074604 -0.00118606]\n",
      "Action: Don't accelerate\n",
      "[-6.6093224e-01 -1.8623266e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-6.6111737e-01 -1.8512466e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.66030014  0.00081726]\n",
      "Action: Don't accelerate\n",
      "[-0.6584861   0.00181402]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6546878   0.00379829]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6489315   0.00575631]\n",
      "Action: Don't accelerate\n",
      "[-0.6422572   0.00667431]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6357116   0.00654557]\n",
      "Action: Don't accelerate\n",
      "[-0.62834096  0.00737066]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6191976   0.00914336]\n",
      "Action: Accelerate to the Left\n",
      "[-0.61034703  0.00885056]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5998532   0.01049385]\n",
      "Action: Accelerate to the Left\n",
      "[-0.58979243  0.01006079]\n",
      "Action: Accelerate to the Left\n",
      "[-0.58023846  0.00955398]\n",
      "Action: Don't accelerate\n",
      "[-0.5702617   0.00997674]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56093615  0.00932557]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5503311   0.01060501]\n",
      "Action: Don't accelerate\n",
      "[-0.5395258   0.01080529]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5296011  0.0099247]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5186314   0.01096971]\n",
      "Action: Don't accelerate\n",
      "[-0.50769895  0.01093246]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4978857   0.00981326]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4872651  0.0106206]\n",
      "Action: Accelerate to the Left\n",
      "[-0.47791645  0.00934864]\n",
      "Action: Don't accelerate\n",
      "[-0.46890935  0.00900709]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4613106   0.00759876]\n",
      "Action: Don't accelerate\n",
      "[-0.4541763   0.00713431]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4465589  0.0076174]\n",
      "Action: Accelerate to the Left\n",
      "[-0.44051418  0.00604473]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43408614  0.00642803]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4293214   0.00476472]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4262544   0.00306703]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42490712  0.00134728]\n",
      "Action: Don't accelerate\n",
      "[-0.42428926  0.00061786]\n",
      "Action: Don't accelerate\n",
      "[-4.24405247e-01 -1.15990835e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.42525426 -0.00084901]\n",
      "Action: Don't accelerate\n",
      "[-0.4268302  -0.00157594]\n",
      "Action: Don't accelerate\n",
      "[-0.42912173 -0.00229155]\n",
      "Action: Don't accelerate\n",
      "[-0.43211243 -0.00299068]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4367807  -0.00466825]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44109273 -0.00431205]\n",
      "Action: Don't accelerate\n",
      "[-0.44601727 -0.00492454]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45251843 -0.00650117]\n",
      "Action: Accelerate to the Right\n",
      "[-0.45854867 -0.00603024]\n",
      "Action: Don't accelerate\n",
      "[-0.4650637  -0.00651502]\n",
      "Action: Don't accelerate\n",
      "[-0.47201547 -0.00695178]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47835258 -0.00633712]\n",
      "Action: Accelerate to the Right\n",
      "[-0.484028   -0.00567542]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49099952 -0.0069715 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49721512 -0.00621561]\n",
      "Action: Don't accelerate\n",
      "[-0.50362843 -0.00641328]\n",
      "Action: Don't accelerate\n",
      "[-0.5101914  -0.00656297]\n",
      "Action: Don't accelerate\n",
      "[-0.5168549 -0.0066635]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5245689  -0.00771407]\n",
      "Action: Don't accelerate\n",
      "[-0.53227574 -0.0077068 ]\n",
      "Action: Don't accelerate\n",
      "[-0.53991747 -0.00764173]\n",
      "Action: Don't accelerate\n",
      "[-0.54743683 -0.00751939]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5557776  -0.00834075]\n",
      "Action: Don't accelerate\n",
      "[-0.5638774  -0.00809979]\n",
      "Action: Don't accelerate\n",
      "[-0.57167584 -0.00779843]\n",
      "Action: Don't accelerate\n",
      "[-0.56729674  0.        ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56797   -0.0006732]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56731135  0.0006586 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56532586  0.0019855 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5620282   0.00329764]\n",
      "Action: Accelerate to the Left\n",
      "[-0.559443    0.00258522]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5575895   0.00185354]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55448145  0.00310803]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5521421   0.00233931]\n",
      "Action: Don't accelerate\n",
      "[-0.549589    0.00255313]\n",
      "Action: Don't accelerate\n",
      "[-0.54684114  0.00274786]\n",
      "Action: Don't accelerate\n",
      "[-0.5439191   0.00292203]\n",
      "Action: Don't accelerate\n",
      "[-0.54084474  0.00307434]\n",
      "Action: Don't accelerate\n",
      "[-0.5376411   0.00320363]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5333322   0.00430891]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5299503  0.0033819]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5275208   0.00242954]\n",
      "Action: Don't accelerate\n",
      "[-0.5250618   0.00245895]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5215919   0.00346993]\n",
      "Action: Accelerate to the Right\n",
      "[-0.517137    0.00445487]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5117306   0.00540641]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5054132   0.00631742]\n",
      "Action: Don't accelerate\n",
      "[-0.49923208  0.0061811 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49223357  0.00699851]\n",
      "Action: Don't accelerate\n",
      "[-0.48546997  0.00676362]\n",
      "Action: Don't accelerate\n",
      "[-0.4789917   0.00647827]\n",
      "Action: Don't accelerate\n",
      "[-0.47284696  0.00614472]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4660814   0.00676555]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4607451   0.00533631]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4548774   0.00586769]\n",
      "Action: Don't accelerate\n",
      "[-0.44952148  0.00535593]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4437166   0.00580491]\n",
      "Action: Don't accelerate\n",
      "[-0.43850508  0.00521151]\n",
      "Action: Accelerate to the Left\n",
      "[-0.43492487  0.00358022]\n",
      "Action: Accelerate to the Left\n",
      "[-0.43300188  0.00192298]\n",
      "Action: Accelerate to the Left\n",
      "[-4.3275005e-01  2.5183544e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.3317118e-01 -4.2112585e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-4.3326220e-01 -9.1044756e-05]\n",
      "Action: Accelerate to the Left\n",
      "[-0.43502253 -0.00176031]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43643937 -0.00141684]\n",
      "Action: Don't accelerate\n",
      "[-0.43850246 -0.00206311]\n",
      "Action: Accelerate to the Left\n",
      "[-0.44219688 -0.00369442]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44549578 -0.00329889]\n",
      "Action: Don't accelerate\n",
      "[-0.4493751  -0.00387931]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4528065 -0.0034314]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45776486 -0.00495836]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46221375 -0.00444891]\n",
      "Action: Accelerate to the Left\n",
      "[-0.46812046 -0.0059067 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47344133 -0.00532087]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47813696 -0.00469563]\n",
      "Action: Don't accelerate\n",
      "[-0.4831725  -0.00503554]\n",
      "Action: Don't accelerate\n",
      "[-0.48851052 -0.005338  ]\n",
      "Action: Don't accelerate\n",
      "[-0.49411118 -0.00560067]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49893272 -0.00482154]\n",
      "Action: Don't accelerate\n",
      "[-0.5039391  -0.00500637]\n",
      "Action: Don't accelerate\n",
      "[-0.5090928  -0.00515373]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5133553  -0.00426249]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51869464 -0.0053393 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52507067 -0.00637608]\n",
      "Action: Don't accelerate\n",
      "[-0.5314357  -0.00636504]\n",
      "Action: Accelerate to the Left\n",
      "[-0.538742   -0.00730627]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5469347  -0.00819274]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55395263 -0.00701786]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5597431  -0.00579052]\n",
      "Action: Don't accelerate\n",
      "[-0.5652631  -0.00551997]\n",
      "Action: Don't accelerate\n",
      "[-0.5704714 -0.0052083]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5743293  -0.00385791]\n",
      "Action: Don't accelerate\n",
      "[-0.5778082 -0.0034789]\n",
      "Action: Don't accelerate\n",
      "[-0.5808823  -0.00307412]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5845289 -0.0036466]\n",
      "Action: Don't accelerate\n",
      "[-0.5877211  -0.00319217]\n",
      "Action: Don't accelerate\n",
      "[-0.5904353  -0.00271421]\n",
      "Action: Don't accelerate\n",
      "[-0.5926516  -0.00221629]\n",
      "Action: Don't accelerate\n",
      "[-0.5943537  -0.00170209]\n",
      "Action: Don't accelerate\n",
      "[-0.5955291 -0.0011754]\n",
      "Action: Don't accelerate\n",
      "[-0.59616923 -0.0006401 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5952693   0.00089988]\n",
      "Action: Accelerate to the Right\n",
      "[-0.592836    0.00243328]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5908872   0.00194884]\n",
      "Action: Accelerate to the Right\n",
      "[-0.58743715  0.00345008]\n",
      "Action: Don't accelerate\n",
      "[-0.5835112   0.00392594]\n",
      "Action: Don't accelerate\n",
      "[-0.57913834  0.00437287]\n",
      "Action: Don't accelerate\n",
      "[-0.57435083  0.00478749]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56818414  0.00616667]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5606841   0.00750006]\n",
      "Action: Don't accelerate\n",
      "[-0.55290645  0.00777763]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54590935  0.00699715]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53774494  0.00816435]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52847457  0.00927042]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51816756  0.01030699]\n",
      "Action: Don't accelerate\n",
      "[-0.5079013   0.01026625]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49875274  0.00914857]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48879033  0.00996239]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47808853  0.0107018 ]\n",
      "Action: Accelerate to the Left\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.468727    0.00936154]\n",
      "Action: Don't accelerate\n",
      "[-0.45977515  0.00895185]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45229906  0.0074761 ]\n",
      "Action: Don't accelerate\n",
      "[-0.44535363  0.00694542]\n",
      "Action: Don't accelerate\n",
      "[-0.43898967  0.00636395]\n",
      "Action: Don't accelerate\n",
      "[-0.4332535   0.00573618]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40712723  0.        ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4089831  -0.00185587]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41268176 -0.00369865]\n",
      "Action: Accelerate to the Left\n",
      "[-0.418197   -0.00551526]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42348966 -0.00529266]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4305219  -0.00703225]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4372432  -0.00672129]\n",
      "Action: Don't accelerate\n",
      "[-0.44460493 -0.00736173]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4535536  -0.00894866]\n",
      "Action: Don't accelerate\n",
      "[-0.46302375 -0.00947014]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4739457  -0.01092196]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48623866 -0.01229298]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49781126 -0.01157259]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5105771  -0.01276581]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5244405  -0.01386344]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53929764 -0.01485713]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5550371  -0.01573943]\n",
      "Action: Don't accelerate\n",
      "[-0.5705411 -0.015504 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.58469415 -0.01415309]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5993916  -0.01469744]\n",
      "Action: Don't accelerate\n",
      "[-0.61352545 -0.01413388]\n",
      "Action: Don't accelerate\n",
      "[-0.62699306 -0.01346758]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63869756 -0.0117045 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.65055585 -0.01185831]\n",
      "Action: Don't accelerate\n",
      "[-0.66148484 -0.01092898]\n",
      "Action: Don't accelerate\n",
      "[-0.6714089  -0.00992408]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6812604  -0.00985148]\n",
      "Action: Accelerate to the Right\n",
      "[-0.688973   -0.00771261]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6964956  -0.00752257]\n",
      "Action: Accelerate to the Right\n",
      "[-0.70177877 -0.00528321]\n",
      "Action: Accelerate to the Left\n",
      "[-0.70678836 -0.00500959]\n",
      "Action: Don't accelerate\n",
      "[-0.71049213 -0.0037038 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.71386653 -0.00337439]\n",
      "Action: Accelerate to the Left\n",
      "[-0.71689016 -0.00302362]\n",
      "Action: Accelerate to the Left\n",
      "[-0.719544   -0.00265382]\n",
      "Action: Accelerate to the Right\n",
      "[-7.1981144e-01 -2.6741897e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.71869075  0.00112065]\n",
      "Action: Accelerate to the Right\n",
      "[-0.71518904  0.00350173]\n",
      "Action: Don't accelerate\n",
      "[-0.7103282   0.00486083]\n",
      "Action: Accelerate to the Right\n",
      "[-0.703139   0.0071892]\n",
      "Action: Accelerate to the Left\n",
      "[-0.69566745  0.00747158]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6879619   0.00770554]\n",
      "Action: Accelerate to the Left\n",
      "[-0.68007296  0.00788891]\n",
      "Action: Accelerate to the Left\n",
      "[-0.67205316  0.00801984]\n",
      "Action: Accelerate to the Left\n",
      "[-0.66395634  0.0080968 ]\n",
      "Action: Don't accelerate\n",
      "[-0.65483767  0.00911865]\n",
      "Action: Accelerate to the Left\n",
      "[-0.64575994  0.00907771]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6367864   0.00897355]\n",
      "Action: Don't accelerate\n",
      "[-0.6269802   0.00980624]\n",
      "Action: Accelerate to the Right\n",
      "[-0.615411    0.01156923]\n",
      "Action: Don't accelerate\n",
      "[-0.6031618   0.01224914]\n",
      "Action: Don't accelerate\n",
      "[-0.5903216   0.01284022]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5759843  0.0143373]\n",
      "Action: Don't accelerate\n",
      "[-0.5612557   0.01472858]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5452453   0.01601041]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52807266  0.01717264]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51186645  0.01620619]\n",
      "Action: Don't accelerate\n",
      "[-0.49574825  0.01611822]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47883868  0.01690958]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46126378  0.01757489]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44315368  0.0181101 ]\n",
      "Action: Don't accelerate\n",
      "[-0.4256411  0.0175126]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40985265  0.01578845]\n",
      "Action: Don't accelerate\n",
      "[-0.39490083  0.01495181]\n",
      "Action: Accelerate to the Left\n",
      "[-0.38189045  0.01301038]\n",
      "Action: Accelerate to the Left\n",
      "[-0.37091115  0.01097929]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3620374   0.00887376]\n",
      "Action: Accelerate to the Right\n",
      "[-0.35332847  0.00870894]\n",
      "Action: Don't accelerate\n",
      "[-0.34584174  0.00748673]\n",
      "Action: Accelerate to the Left\n",
      "[-0.34062588  0.00521585]\n",
      "Action: Don't accelerate\n",
      "[-0.33671445  0.00391143]\n",
      "Action: Accelerate to the Left\n",
      "[-0.33513236  0.00158209]\n",
      "Action: Accelerate to the Right\n",
      "[-0.33388966  0.0012427 ]\n",
      "Action: Don't accelerate\n",
      "[-3.3399421e-01 -1.0453989e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.33444533 -0.00045112]\n",
      "Action: Accelerate to the Right\n",
      "[-0.3352402  -0.00079485]\n",
      "Action: Accelerate to the Right\n",
      "[-0.33637372 -0.00113355]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3398388  -0.00346506]\n",
      "Action: Accelerate to the Right\n",
      "[-0.3436133  -0.00377451]\n",
      "Action: Don't accelerate\n",
      "[-0.34867305 -0.00505976]\n",
      "Action: Accelerate to the Left\n",
      "[-0.35598537 -0.00731231]\n",
      "Action: Don't accelerate\n",
      "[-0.36450246 -0.0085171 ]\n",
      "Action: Don't accelerate\n",
      "[-0.37416798 -0.00966553]\n",
      "Action: Accelerate to the Right\n",
      "[-0.3839171 -0.0097491]\n",
      "Action: Don't accelerate\n",
      "[-0.3946834  -0.01076632]\n",
      "Action: Accelerate to the Right\n",
      "[-0.40539265 -0.01070926]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41797    -0.01257734]\n",
      "Action: Don't accelerate\n",
      "[-0.43132636 -0.01335636]\n",
      "Action: Don't accelerate\n",
      "[-0.44536597 -0.0140396 ]\n",
      "Action: Don't accelerate\n",
      "[-0.45998695 -0.01462098]\n",
      "Action: Accelerate to the Left\n",
      "[-0.47608212 -0.01609518]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49153247 -0.01545034]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50822294 -0.01669047]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52402866 -0.01580575]\n",
      "Action: Don't accelerate\n",
      "[-0.5398312  -0.01580252]\n",
      "Action: Accelerate to the Right\n",
      "[-0.554512   -0.01468083]\n",
      "Action: Don't accelerate\n",
      "[-0.5689614  -0.01444931]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5840715  -0.01511014]\n",
      "Action: Accelerate to the Left\n",
      "[-0.59973055 -0.01565908]\n",
      "Action: Don't accelerate\n",
      "[-0.61482364 -0.01509304]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56125134  0.        ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55996954  0.00128179]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55741554  0.00255403]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5536083   0.00380722]\n",
      "Action: Don't accelerate\n",
      "[-0.5495763   0.00403199]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5443497   0.00522663]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5379675   0.00638216]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53047764  0.00748989]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5219362   0.00854148]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51440716  0.00752901]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50594705  0.00846008]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49861932  0.00732775]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49247873  0.00614058]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4855712   0.00690752]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47794828  0.00762293]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46966666  0.00828162]\n",
      "Action: Accelerate to the Right\n",
      "[-0.46078774  0.0088789 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.45137715  0.0094106 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.441504    0.00987316]\n",
      "Action: Accelerate to the Left\n",
      "[-0.43324035  0.00826366]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4246461   0.00859424]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41778317  0.00686294]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41270056  0.00508259]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40943447  0.00326611]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40800795  0.00142652]\n",
      "Action: Accelerate to the Right\n",
      "[-0.40643108  0.00157686]\n",
      "Action: Accelerate to the Left\n",
      "[-4.0671501e-01 -2.8391316e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40885767 -0.00214269]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41284403 -0.00398635]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41864583 -0.00580181]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42622185 -0.00757602]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43351787 -0.007296  ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44048128 -0.00696341]\n",
      "Action: Accelerate to the Left\n",
      "[-0.44906163 -0.00858036]\n",
      "Action: Accelerate to the Right\n",
      "[-0.45719635 -0.00813474]\n",
      "Action: Accelerate to the Left\n",
      "[-0.46682584 -0.00962947]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47587904 -0.00905321]\n",
      "Action: Don't accelerate\n",
      "[-0.48528892 -0.00940989]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4959855  -0.01069658]\n",
      "Action: Accelerate to the Left\n",
      "[-0.507889   -0.01190344]\n",
      "Action: Don't accelerate\n",
      "[-0.51991016 -0.01202122]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53295904 -0.01304888]\n",
      "Action: Don't accelerate\n",
      "[-0.5459377  -0.01297869]\n",
      "Action: Don't accelerate\n",
      "[-0.558749   -0.01281128]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5702972  -0.01154814]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5804962  -0.01019904]\n",
      "Action: Don't accelerate\n",
      "[-0.5902706  -0.00977438]\n",
      "Action: Don't accelerate\n",
      "[-0.5995483  -0.00927767]\n",
      "Action: Don't accelerate\n",
      "[-0.6082612  -0.00871297]\n",
      "Action: Don't accelerate\n",
      "[-0.616346  -0.0080848]\n",
      "Action: Accelerate to the Right\n",
      "[-0.62274414 -0.00639814]\n",
      "Action: Don't accelerate\n",
      "[-0.6284096  -0.00566546]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63230187 -0.00389227]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6343933  -0.00209137]\n",
      "Action: Don't accelerate\n",
      "[-0.6356689  -0.00127563]\n",
      "Action: Don't accelerate\n",
      "[-6.3611972e-01 -4.5084927e-04]\n",
      "Action: Don't accelerate\n",
      "[-6.357426e-01  3.771220e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6335402   0.00220242]\n",
      "Action: Don't accelerate\n",
      "[-0.6305281   0.00301212]\n",
      "Action: Accelerate to the Right\n",
      "[-0.62572765  0.0048004 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.62117326  0.00455444]\n",
      "Action: Don't accelerate\n",
      "[-0.6158974   0.00527584]\n",
      "Action: Accelerate to the Left\n",
      "[-0.61093813  0.00495927]\n",
      "Action: Accelerate to the Right\n",
      "[-0.60433125  0.00660685]\n",
      "Action: Don't accelerate\n",
      "[-0.5971248   0.00720644]\n",
      "Action: Don't accelerate\n",
      "[-0.58937144  0.00775342]\n",
      "Action: Accelerate to the Left\n",
      "[-0.58212787  0.00724352]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5754477   0.00668024]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56738013  0.00806754]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5579852   0.00939496]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54733276  0.0106524 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5375025   0.00983025]\n",
      "Action: Accelerate to the Right\n",
      "[-0.526568   0.0109345]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51461124  0.01195677]\n",
      "Action: Don't accelerate\n",
      "[-0.5027219   0.01188937]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48998898  0.0127329 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47650772  0.01348125]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4643785   0.01212924]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4516911   0.01268742]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4405388   0.01115229]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42900303  0.01153576]\n",
      "Action: Accelerate to the Right\n",
      "[-0.41716725  0.01183578]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4071162   0.01005104]\n",
      "Action: Don't accelerate\n",
      "[-0.39792112  0.00919509]\n",
      "Action: Accelerate to the Right\n",
      "[-0.38864645  0.00927468]\n",
      "Action: Accelerate to the Right\n",
      "[-0.37935647  0.00928996]\n",
      "Action: Accelerate to the Right\n",
      "[-0.3701149   0.00924159]\n",
      "Action: Accelerate to the Left\n",
      "[-0.36298418  0.00713071]\n",
      "Action: Don't accelerate\n",
      "[-0.357012    0.00597218]\n",
      "Action: Don't accelerate\n",
      "[-0.35223785  0.00477414]\n",
      "Action: Accelerate to the Right\n",
      "[-0.34769306  0.0045448 ]\n",
      "Action: Don't accelerate\n",
      "[-0.34440717  0.00328589]\n",
      "Action: Don't accelerate\n",
      "[-0.34240142  0.00200576]\n",
      "Action: Don't accelerate\n",
      "[-0.3416887   0.00071272]\n",
      "Action: Accelerate to the Right\n",
      "[-0.34127358  0.00041512]\n",
      "Action: Accelerate to the Right\n",
      "[-3.41158748e-01  1.14849274e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3433449  -0.00218615]\n",
      "Action: Accelerate to the Left\n",
      "[-0.34781802 -0.00447313]\n",
      "Action: Accelerate to the Right\n",
      "[-0.35254923 -0.00473122]\n",
      "Action: Accelerate to the Right\n",
      "[-0.35750777 -0.00495853]\n",
      "Action: Don't accelerate\n",
      "[-0.49187183  0.        ]\n",
      "Action: Don't accelerate\n",
      "[-4.9210945e-01 -2.3759287e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.9258286e-01 -4.7341181e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49428853 -0.0017057 ]\n",
      "Action: Don't accelerate\n",
      "[-0.4962138  -0.00192524]\n",
      "Action: Don't accelerate\n",
      "[-0.49834418 -0.0021304 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5016638  -0.00331962]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5041478  -0.00248402]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50577766 -0.00162982]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5065411  -0.00076341]\n",
      "Action: Accelerate to the Right\n",
      "[-5.0643235e-01  1.0871093e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.0645232e-01 -1.9980445e-05]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50560087  0.00085148]\n",
      "Action: Don't accelerate\n",
      "[-0.5048843   0.00071656]\n",
      "Action: Accelerate to the Left\n",
      "[-5.0530803e-01 -4.2372607e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.50586885 -0.00056084]\n",
      "Action: Don't accelerate\n",
      "[-0.5065626  -0.00069375]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50838405 -0.00182147]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5113196  -0.00293554]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5133472  -0.00202761]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5164517  -0.00310448]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51860976 -0.00215808]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5218053 -0.0031955]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52401423 -0.00220895]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5272201  -0.00320583]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5313987  -0.00417868]\n",
      "Action: Don't accelerate\n",
      "[-0.5355189  -0.00412018]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5385497 -0.0030308]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5424684 -0.0039187]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54524565 -0.00277726]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5468607  -0.00161502]\n",
      "Action: Don't accelerate\n",
      "[-0.5483014 -0.0014407]\n",
      "Action: Accelerate to the Right\n",
      "[-5.485570e-01 -2.555972e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5496256  -0.00106859]\n",
      "Action: Accelerate to the Right\n",
      "[-5.4949915e-01  1.2641726e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5481787   0.00132047]\n",
      "Action: Accelerate to the Right\n",
      "[-0.545674    0.00250466]\n",
      "Action: Don't accelerate\n",
      "[-0.5430039  0.0026701]\n",
      "Action: Don't accelerate\n",
      "[-0.5401884   0.00281556]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53624845  0.00393993]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53121364  0.00503478]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5271218   0.00409188]\n",
      "Action: Don't accelerate\n",
      "[-0.52300346  0.00411831]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51988965  0.00311384]\n",
      "Action: Don't accelerate\n",
      "[-0.5168036   0.00308602]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51276857  0.00403506]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5078147   0.00495385]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5039792   0.00383551]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50129074  0.00268845]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49776947  0.00352127]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49544173  0.00232774]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49432492  0.00111681]\n",
      "Action: Don't accelerate\n",
      "[-0.49342737  0.00089754]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4917558   0.00167157]\n",
      "Action: Accelerate to the Left\n",
      "[-4.913227e-01  4.331060e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4921313  -0.00080859]\n",
      "Action: Accelerate to the Right\n",
      "[-4.9217552e-01 -4.4242315e-05]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49345508 -0.00127957]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49396044 -0.00050534]\n",
      "Action: Accelerate to the Right\n",
      "[-4.9368775e-01  2.7266779e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.9363914e-01  4.8636135e-05]\n",
      "Action: Don't accelerate\n",
      "[-4.9381489e-01 -1.7575883e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.9421373e-01 -3.9884090e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-4.9383268e-01  3.8105657e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49267456  0.00115811]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49074805  0.00192651]\n",
      "Action: Don't accelerate\n",
      "[-0.48906752  0.00168053]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48664552  0.00242201]\n",
      "Action: Don't accelerate\n",
      "[-0.4845001   0.00214542]\n",
      "Action: Don't accelerate\n",
      "[-0.48264724  0.00185285]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48210075  0.00054649]\n",
      "Action: Don't accelerate\n",
      "[-4.818647e-01  2.360587e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48294082 -0.00107613]\n",
      "Action: Accelerate to the Right\n",
      "[-4.8332113e-01 -3.8030886e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.4840028  -0.00068166]\n",
      "Action: Don't accelerate\n",
      "[-0.48498073 -0.00097793]\n",
      "Action: Accelerate to the Right\n",
      "[-4.8524764e-01 -2.6691653e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-4.8480156e-01  4.4608407e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.8464578e-01  1.5576143e-04]\n",
      "Action: Don't accelerate\n",
      "[-4.847815e-01 -1.357215e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4862077  -0.00142619]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48891374 -0.00270604]\n",
      "Action: Don't accelerate\n",
      "[-0.49187946 -0.00296571]\n",
      "Action: Don't accelerate\n",
      "[-0.4950827  -0.00320324]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49749956 -0.00241685]\n",
      "Action: Don't accelerate\n",
      "[-0.50011194 -0.0026124 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.50390035 -0.0037884 ]\n",
      "Action: Don't accelerate\n",
      "[-0.5078364  -0.00393606]\n",
      "Action: Don't accelerate\n",
      "[-0.51189065 -0.00405423]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5170327  -0.00514202]\n",
      "Action: Don't accelerate\n",
      "[-0.5222239  -0.00519126]\n",
      "Action: Don't accelerate\n",
      "[-0.52742547 -0.00520158]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5315984  -0.00417288]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5367113  -0.00511289]\n",
      "Action: Don't accelerate\n",
      "[-0.5417258  -0.00501457]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5456045  -0.00387868]\n",
      "Action: Accelerate to the Left\n",
      "[-0.55031824 -0.00471376]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5538318  -0.00351358]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55611897 -0.00228714]\n",
      "Action: Don't accelerate\n",
      "[-0.5581626  -0.00204362]\n",
      "Action: Don't accelerate\n",
      "[-0.42228195  0.        ]\n",
      "Action: Accelerate to the Right\n",
      "[-4.2203018e-01  2.5177028e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42152846  0.00050174]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42278033 -0.00125188]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42377687 -0.00099654]\n",
      "Action: Don't accelerate\n",
      "[-0.42551094 -0.00173407]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4289701  -0.00345916]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43212947 -0.00315938]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4369663  -0.00483682]\n",
      "Action: Accelerate to the Left\n",
      "[-0.44344556 -0.00647927]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4495202  -0.00607465]\n",
      "Action: Don't accelerate\n",
      "[-0.45614588 -0.00662567]\n",
      "Action: Don't accelerate\n",
      "[-0.463274   -0.00712812]\n",
      "Action: Don't accelerate\n",
      "[-0.4708521  -0.00757809]\n",
      "Action: Accelerate to the Left\n",
      "[-0.47982416 -0.00897204]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49012357 -0.01029941]\n",
      "Action: Don't accelerate\n",
      "[-0.5006736  -0.01055005]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51239544 -0.01172186]\n",
      "Action: Don't accelerate\n",
      "[-0.52420133 -0.01180587]\n",
      "Action: Don't accelerate\n",
      "[-0.5360027  -0.01180135]\n",
      "Action: Don't accelerate\n",
      "[-0.547711   -0.01170834]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5582387  -0.01052765]\n",
      "Action: Accelerate to the Right\n",
      "[-0.56750697 -0.00926832]\n",
      "Action: Don't accelerate\n",
      "[-0.57644695 -0.00893996]\n",
      "Action: Don't accelerate\n",
      "[-0.58499223 -0.00854526]\n",
      "Action: Don't accelerate\n",
      "[-0.5930796  -0.00808741]\n",
      "Action: Don't accelerate\n",
      "[-0.6006497  -0.00757007]\n",
      "Action: Accelerate to the Left\n",
      "[-0.608647   -0.00799732]\n",
      "Action: Accelerate to the Left\n",
      "[-0.61701334 -0.00836635]\n",
      "Action: Don't accelerate\n",
      "[-0.62468827 -0.00767488]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6326165  -0.00792828]\n",
      "Action: Don't accelerate\n",
      "[-0.63974166 -0.00712514]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6450133  -0.00527159]\n",
      "Action: Don't accelerate\n",
      "[-0.6493942  -0.00438098]\n",
      "Action: Don't accelerate\n",
      "[-0.65285397 -0.00345975]\n",
      "Action: Accelerate to the Left\n",
      "[-0.65636843 -0.00351445]\n",
      "Action: Don't accelerate\n",
      "[-0.65891325 -0.0025448 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.66147083 -0.00255759]\n",
      "Action: Don't accelerate\n",
      "[-0.6630236  -0.00155278]\n",
      "Action: Don't accelerate\n",
      "[-6.635609e-01 -5.373168e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6620791   0.00148183]\n",
      "Action: Don't accelerate\n",
      "[-0.6595883   0.00249081]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6551056   0.00448267]\n",
      "Action: Don't accelerate\n",
      "[-0.649662    0.00544359]\n",
      "Action: Don't accelerate\n",
      "[-0.64329535  0.00636668]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6370501   0.00624524]\n",
      "Action: Don't accelerate\n",
      "[-0.6299703   0.00707979]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6211062  0.0088641]\n",
      "Action: Don't accelerate\n",
      "[-0.6115212   0.00958503]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6022844   0.00923682]\n",
      "Action: Accelerate to the Right\n",
      "[-0.59146285  0.0108215 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5791359   0.01232697]\n",
      "Action: Don't accelerate\n",
      "[-0.5663943   0.01274158]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5543327   0.01206166]\n",
      "Action: Don't accelerate\n",
      "[-0.5420408   0.01229184]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5286107   0.01343008]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51414305  0.01446767]\n",
      "Action: Don't accelerate\n",
      "[-0.4997463   0.01439676]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48452827  0.01521802]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4686026   0.01592566]\n",
      "Action: Don't accelerate\n",
      "[-0.45308754  0.01551506]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43709737  0.01599016]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4227487   0.01434866]\n",
      "Action: Don't accelerate\n",
      "[-0.40914494  0.01360377]\n",
      "Action: Accelerate to the Left\n",
      "[-0.39738283  0.01176213]\n",
      "Action: Accelerate to the Left\n",
      "[-0.38754484  0.00983797]\n",
      "Action: Don't accelerate\n",
      "[-0.37869918  0.00884567]\n",
      "Action: Don't accelerate\n",
      "[-0.37090635  0.00779282]\n",
      "Action: Don't accelerate\n",
      "[-0.3642191   0.00668726]\n",
      "Action: Accelerate to the Left\n",
      "[-0.35968214  0.00453694]\n",
      "Action: Don't accelerate\n",
      "[-0.35632563  0.00335652]\n",
      "Action: Don't accelerate\n",
      "[-0.35417166  0.00215397]\n",
      "Action: Accelerate to the Right\n",
      "[-0.3522344   0.00193727]\n",
      "Action: Accelerate to the Left\n",
      "[-3.5252649e-01 -2.9208878e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.35404602 -0.00151954]\n",
      "Action: Accelerate to the Left\n",
      "[-0.35778308 -0.00373706]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3637131  -0.00593002]\n",
      "Action: Don't accelerate\n",
      "[-0.3707968  -0.00708371]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3799868 -0.00919  ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.3912209  -0.01123408]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4024219  -0.01120102]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41551188 -0.01308996]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42839834 -0.01288648]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44098917 -0.01259082]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45519322 -0.01420407]\n",
      "Action: Accelerate to the Left\n",
      "[-0.47090676 -0.01571352]\n",
      "Action: Don't accelerate\n",
      "[-0.48701382 -0.01610706]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5043947 -0.0173809]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52291954 -0.01852485]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5404495  -0.01752994]\n",
      "Action: Accelerate to the Left\n",
      "[-0.55885315 -0.01840362]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5759928 -0.0171397]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5937412  -0.01774836]\n",
      "Action: Accelerate to the Right\n",
      "[-0.60996735 -0.01622617]\n",
      "Action: Don't accelerate\n",
      "[-0.625553   -0.01558563]\n",
      "Action: Don't accelerate\n",
      "[-0.6403858  -0.01483284]\n",
      "Action: Accelerate to the Right\n",
      "[-0.65336055 -0.01297475]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6663865  -0.01302593]\n",
      "Action: Accelerate to the Right\n",
      "[-0.67737395 -0.01098747]\n",
      "Action: Don't accelerate\n",
      "[-0.6872486  -0.00987463]\n",
      "Action: Don't accelerate\n",
      "[-0.54625577  0.        ]\n",
      "Action: Don't accelerate\n",
      "[-5.4608595e-01  1.6979658e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5467476  -0.00066168]\n",
      "Action: Don't accelerate\n",
      "[-5.4723585e-01 -4.8820043e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.4754692e-01 -3.1107097e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54667854  0.00086839]\n",
      "Action: Don't accelerate\n",
      "[-0.5456372   0.00104135]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5434307   0.00220651]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5420755   0.00135516]\n",
      "Action: Accelerate to the Left\n",
      "[-5.4158181e-01  4.9366907e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.4195338e-01 -3.7152338e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.4218727e-01 -2.3393356e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5432819  -0.00109459]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54522896 -0.00194705]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5460139  -0.00078494]\n",
      "Action: Don't accelerate\n",
      "[-0.54663086 -0.00061696]\n",
      "Action: Don't accelerate\n",
      "[-5.4707521e-01 -4.4435236e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5483436  -0.00126842]\n",
      "Action: Don't accelerate\n",
      "[-0.5494266  -0.00108301]\n",
      "Action: Accelerate to the Right\n",
      "[-5.4931611e-01  1.1050616e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5480129   0.00130319]\n",
      "Action: Don't accelerate\n",
      "[-0.5465268   0.00148614]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54386884  0.00265796]\n",
      "Action: Don't accelerate\n",
      "[-0.54105896  0.00280989]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5391182   0.00194078]\n",
      "Action: Don't accelerate\n",
      "[-0.537061    0.00205714]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5359029   0.00115808]\n",
      "Action: Don't accelerate\n",
      "[-0.5346526   0.00125034]\n",
      "Action: Don't accelerate\n",
      "[-0.53331935  0.00133323]\n",
      "Action: Accelerate to the Left\n",
      "[-5.3291327e-01  4.0611994e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-5.3343725e-01 -5.2403059e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5328875   0.00054975]\n",
      "Action: Accelerate to the Left\n",
      "[-5.3326815e-01 -3.8059594e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.3357619e-01 -3.0808602e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5328095   0.00076673]\n",
      "Action: Accelerate to the Left\n",
      "[-5.3297365e-01 -1.6419496e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53206754  0.00090611]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53009796  0.00196962]\n",
      "Action: Don't accelerate\n",
      "[-0.52807957  0.00201836]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5270276   0.00105196]\n",
      "Action: Accelerate to the Left\n",
      "[-5.2694994e-01  7.7678305e-05]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5278471  -0.00089719]\n",
      "Action: Accelerate to the Right\n",
      "[-5.2771246e-01  1.3467354e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.2754694e-01  1.6552542e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.2735180e-01  1.9513594e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.2712852e-01  2.2328306e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.2687877e-01  2.4975566e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5256044   0.00127436]\n",
      "Action: Accelerate to the Right\n",
      "[-0.523315   0.0022894]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52202773  0.00128727]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51975226  0.00227549]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51850563  0.00124664]\n",
      "Action: Don't accelerate\n",
      "[-0.51729715  0.00120844]\n",
      "Action: Accelerate to the Left\n",
      "[-5.1713598e-01  1.6118176e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.1702327e-01  1.1271395e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5179599 -0.0009366]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51993877 -0.00197889]\n",
      "Action: Don't accelerate\n",
      "[-0.5219451  -0.00200634]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52496386 -0.00301874]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52697235 -0.0020085 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.52795553 -0.0009832 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5299061  -0.00195053]\n",
      "Action: Don't accelerate\n",
      "[-0.53180933 -0.00190322]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53265095 -0.00084165]\n",
      "Action: Don't accelerate\n",
      "[-0.53342474 -0.00077377]\n",
      "Action: Don't accelerate\n",
      "[-0.5341248  -0.00070008]\n",
      "Action: Don't accelerate\n",
      "[-0.53474593 -0.00062115]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action: Accelerate to the Left\n",
      "[-0.53628355 -0.00153756]\n",
      "Action: Accelerate to the Right\n",
      "[-5.367260e-01 -4.424517e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.3706998e-01 -3.4402314e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53831303 -0.00124302]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5404457 -0.0021327]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5434521 -0.0030064]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5473097  -0.00385759]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5499896 -0.0026799]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55147177 -0.00148218]\n",
      "Action: Accelerate to the Right\n",
      "[-5.5174518e-01 -2.7337487e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.55080765  0.00093747]\n",
      "Action: Accelerate to the Left\n",
      "[-5.5066639e-01  1.4131243e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.5032229e-01  3.4409633e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54877794  0.00154431]\n",
      "Action: Accelerate to the Right\n",
      "[-0.546045    0.00273297]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5441438   0.00190119]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54108864  0.00305518]\n",
      "Action: Don't accelerate\n",
      "[-0.5379023  0.0031863]\n",
      "Action: Don't accelerate\n",
      "[-0.5346088   0.00329354]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53023267  0.0043761 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.52680683  0.00342585]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5243569   0.00244991]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5209013  0.0034556]\n",
      "Action: Accelerate to the Left\n",
      "[-0.51846594  0.00243537]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51506907  0.00339687]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5127362   0.00233291]\n",
      "Action: Don't accelerate\n",
      "[-0.51048476  0.00225145]\n",
      "Action: Don't accelerate\n",
      "[-0.5083316   0.00215312]\n",
      "Action: Don't accelerate\n",
      "[-0.50629294  0.00203866]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50338405  0.00290892]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5016266  0.0017574]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5010339   0.00059273]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49961028  0.00142362]\n",
      "Action: Accelerate to the Left\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        print(observation)\n",
    "        action = env.action_space.sample() #acción aleatoria\n",
    "        if action == 0:\n",
    "            print('Action: Accelerate to the Left')\n",
    "        elif action == 1:\n",
    "            print(\"Action: Don't accelerate\")\n",
    "        else:print('Action: Accelerate to the Right')\n",
    "        observation, reward, done, info = env.step(action) #ejecución de la acción elegida\n",
    "env.close() #cerramos la visualización del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZuCCe4WzJXN"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.2 ptos):</strong> Ejecutar 1000 episodios con el máximo de pasos establecido en el entorno de MountainCar, tomando acciones de forma aleatoria. Almacenar la posición final del coche en cada episodio, y mostrar el resultado en un gráfico. Comentar el resultado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_episode = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53897023  0.        ]\n",
      "Action: Don't accelerate\n",
      "[-5.38854957e-01  1.15244904e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53962535 -0.00077037]\n",
      "Action: Accelerate to the Right\n",
      "[-5.3927559e-01  3.4977932e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.53780824  0.00146731]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5372344   0.00057385]\n",
      "Action: Don't accelerate\n",
      "[-0.53655833  0.00067609]\n",
      "Action: Accelerate to the Left\n",
      "[-5.3678507e-01 -2.2673872e-04]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5379129  -0.00112787]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53993344 -0.00202054]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54283154 -0.00289808]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54658544 -0.00375392]\n",
      "Action: Don't accelerate\n",
      "[-0.55016714 -0.00358165]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5545497 -0.0043826]\n",
      "Action: Don't accelerate\n",
      "[-0.5587005 -0.0041508]\n",
      "Action: Don't accelerate\n",
      "[-0.5625886  -0.00388803]\n",
      "Action: Don't accelerate\n",
      "[-0.5661848  -0.00359627]\n",
      "Action: Don't accelerate\n",
      "[-0.56946254 -0.00327774]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5733974  -0.00393485]\n",
      "Action: Don't accelerate\n",
      "[-0.57696015 -0.00356275]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5811244  -0.00416424]\n",
      "Action: Accelerate to the Left\n",
      "[-0.58585936 -0.00473494]\n",
      "Action: Accelerate to the Right\n",
      "[-0.58913004 -0.0032707 ]\n",
      "Action: Don't accelerate\n",
      "[-0.5919124  -0.00278237]\n",
      "Action: Accelerate to the Right\n",
      "[-0.593186  -0.0012736]\n",
      "Action: Don't accelerate\n",
      "[-0.5939415  -0.00075548]\n",
      "Action: Don't accelerate\n",
      "[-5.9417331e-01 -2.3181333e-04]\n",
      "Action: Don't accelerate\n",
      "[-5.9387976e-01  2.9354988e-04]\n",
      "Action: Accelerate to the Right\n",
      "[-0.592063    0.00181676]\n",
      "Action: Accelerate to the Right\n",
      "[-0.58873636  0.00332664]\n",
      "Action: Don't accelerate\n",
      "[-0.5849243   0.00381207]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5796549   0.00526942]\n",
      "Action: Accelerate to the Right\n",
      "[-0.572967    0.00668786]\n",
      "Action: Don't accelerate\n",
      "[-0.5659102   0.00705677]\n",
      "Action: Don't accelerate\n",
      "[-0.55853695  0.00737325]\n",
      "Action: Accelerate to the Right\n",
      "[-0.54990214  0.00863481]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5400703   0.00983188]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5291149   0.01095537]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51711816  0.01199674]\n",
      "Action: Don't accelerate\n",
      "[-0.50517005  0.01194814]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49236006  0.01280999]\n",
      "Action: Accelerate to the Left\n",
      "[-0.480784    0.01157604]\n",
      "Action: Don't accelerate\n",
      "[-0.4695282   0.01125581]\n",
      "Action: Don't accelerate\n",
      "[-0.45867613  0.01085206]\n",
      "Action: Don't accelerate\n",
      "[-0.4483079   0.01036821]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4394996   0.00880832]\n",
      "Action: Don't accelerate\n",
      "[-0.43131533  0.00818425]\n",
      "Action: Don't accelerate\n",
      "[-0.42381442  0.00750093]\n",
      "Action: Don't accelerate\n",
      "[-0.41705075  0.00676367]\n",
      "Action: Don't accelerate\n",
      "[-0.41107264  0.0059781 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40692255  0.0041501 ]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40462977  0.00229278]\n",
      "Action: Don't accelerate\n",
      "[-0.40321043  0.00141934]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40367448 -0.00046407]\n",
      "Action: Accelerate to the Right\n",
      "[-4.0401873e-01 -3.4422640e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.40524068 -0.00122196]\n",
      "Action: Don't accelerate\n",
      "[-0.4073318  -0.00209111]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41127732 -0.00394554]\n",
      "Action: Accelerate to the Right\n",
      "[-0.41504943 -0.0037721 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.41862133 -0.00357191]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42396763 -0.00534629]\n",
      "Action: Don't accelerate\n",
      "[-0.43005008 -0.00608245]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43582496 -0.00577489]\n",
      "Action: Don't accelerate\n",
      "[-0.44225058 -0.00642561]\n",
      "Action: Accelerate to the Right\n",
      "[-0.44828025 -0.00602968]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45587003 -0.00758978]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4629643  -0.00709425]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4695108  -0.00654651]\n",
      "Action: Accelerate to the Right\n",
      "[-0.47546118 -0.00595039]\n",
      "Action: Don't accelerate\n",
      "[-0.48177135 -0.00631017]\n",
      "Action: Accelerate to the Right\n",
      "[-0.48739442 -0.00562305]\n",
      "Action: Accelerate to the Left\n",
      "[-0.49428847 -0.00689405]\n",
      "Action: Don't accelerate\n",
      "[-0.5014021 -0.0071136]\n",
      "Action: Accelerate to the Left\n",
      "[-0.509682   -0.00827995]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5190663  -0.00938429]\n",
      "Action: Don't accelerate\n",
      "[-0.5284846  -0.00941829]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5388662  -0.01038164]\n",
      "Action: Don't accelerate\n",
      "[-0.5491334  -0.01026718]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5602093  -0.01107586]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5720111  -0.01180183]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5844511  -0.01244001]\n",
      "Action: Accelerate to the Left\n",
      "[-0.59743726 -0.01298615]\n",
      "Action: Accelerate to the Right\n",
      "[-0.60887414 -0.01143688]\n",
      "Action: Don't accelerate\n",
      "[-0.6196784  -0.01080427]\n",
      "Action: Don't accelerate\n",
      "[-0.629772   -0.01009361]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6400827 -0.0103107]\n",
      "Action: Don't accelerate\n",
      "[-0.64953744 -0.00945475]\n",
      "Action: Accelerate to the Left\n",
      "[-0.65906996 -0.00953252]\n",
      "Action: Don't accelerate\n",
      "[-0.6676142  -0.00854422]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6761116 -0.0084974]\n",
      "Action: Don't accelerate\n",
      "[-0.68350464 -0.00739305]\n",
      "Action: Don't accelerate\n",
      "[-0.6897439  -0.00623923]\n",
      "Action: Accelerate to the Right\n",
      "[-0.693788  -0.0040441]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6956104  -0.00182241]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6971992  -0.00158883]\n",
      "Action: Don't accelerate\n",
      "[-6.9754410e-01 -3.4488825e-04]\n",
      "Action: Don't accelerate\n",
      "[-0.6966428   0.00090129]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6935012   0.00314161]\n",
      "Action: Don't accelerate\n",
      "[-0.6891398   0.00436141]\n",
      "Action: Don't accelerate\n",
      "[-0.6835872   0.00555256]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6758803   0.00770693]\n",
      "Action: Don't accelerate\n",
      "[-0.66707057  0.00880973]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6582177   0.00885285]\n",
      "Action: Accelerate to the Right\n",
      "[-0.64738244  0.01083527]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63464     0.01274246]\n",
      "Action: Don't accelerate\n",
      "[-0.62108004  0.01355995]\n",
      "Action: Accelerate to the Left\n",
      "[-0.60779935  0.01328068]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5948939   0.01290549]\n",
      "Action: Don't accelerate\n",
      "[-0.58145773  0.01343614]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5685898  0.0128679]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5563855   0.01220431]\n",
      "Action: Don't accelerate\n",
      "[-0.5439357   0.01244982]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53233343  0.01160225]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5196657   0.01266775]\n",
      "Action: Accelerate to the Right\n",
      "[-0.50602746  0.01363825]\n",
      "Action: Don't accelerate\n",
      "[-0.4925209   0.01350653]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48024714  0.01227378]\n",
      "Action: Don't accelerate\n",
      "[-0.46829757  0.01194956]\n",
      "Action: Don't accelerate\n",
      "[-0.45676085  0.0115367 ]\n",
      "Action: Don't accelerate\n",
      "[-0.4457221   0.01103877]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4362621   0.00945999]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42844966  0.00781244]\n",
      "Action: Don't accelerate\n",
      "[-0.42134118  0.00710847]\n",
      "Action: Don't accelerate\n",
      "[-0.41498768  0.00635351]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41043442  0.00455327]\n",
      "Action: Don't accelerate\n",
      "[-0.40671366  0.00372074]\n",
      "Action: Don't accelerate\n",
      "[-0.40385172  0.00286196]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40286866  0.00098305]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40377143 -0.00090276]\n",
      "Action: Accelerate to the Left\n",
      "[-0.40655366 -0.00278223]\n",
      "Action: Don't accelerate\n",
      "[-0.4101958  -0.00364214]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41567215 -0.00547635]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4209439  -0.00527173]\n",
      "Action: Accelerate to the Left\n",
      "[-0.42797342 -0.00702953]\n",
      "Action: Accelerate to the Right\n",
      "[-0.43471032 -0.00673693]\n",
      "Action: Don't accelerate\n",
      "[-0.44210604 -0.00739572]\n",
      "Action: Don't accelerate\n",
      "[-0.4501069  -0.00800084]\n",
      "Action: Accelerate to the Left\n",
      "[-0.45965448 -0.00954758]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4706787  -0.01102422]\n",
      "Action: Accelerate to the Left\n",
      "[-0.48309815 -0.01241946]\n",
      "Action: Accelerate to the Right\n",
      "[-0.49482062 -0.01172247]\n",
      "Action: Accelerate to the Left\n",
      "[-0.5077586  -0.01293803]\n",
      "Action: Accelerate to the Right\n",
      "[-0.51981544 -0.01205679]\n",
      "Action: Don't accelerate\n",
      "[-0.5319006  -0.01208516]\n",
      "Action: Don't accelerate\n",
      "[-0.5439235  -0.01202291]\n",
      "Action: Don't accelerate\n",
      "[-0.55579406 -0.01187057]\n",
      "Action: Accelerate to the Left\n",
      "[-0.56842357 -0.01262948]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5797179 -0.0112943]\n",
      "Action: Don't accelerate\n",
      "[-0.5905932 -0.0108754]\n",
      "Action: Accelerate to the Left\n",
      "[-0.60196954 -0.01137631]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6137635  -0.01179393]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6238894  -0.01012591]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63227445 -0.00838503]\n",
      "Action: Don't accelerate\n",
      "[-0.6398588  -0.00758433]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6455887  -0.00572995]\n",
      "Action: Accelerate to the Left\n",
      "[-0.651424   -0.00583531]\n",
      "Action: Accelerate to the Right\n",
      "[-0.655324   -0.00389994]\n",
      "Action: Don't accelerate\n",
      "[-0.6582615  -0.00293752]\n",
      "Action: Don't accelerate\n",
      "[-0.6602163 -0.0019548]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6621749  -0.00195861]\n",
      "Action: Don't accelerate\n",
      "[-0.66312385 -0.00094897]\n",
      "Action: Accelerate to the Left\n",
      "[-0.66405666 -0.00093282]\n",
      "Action: Don't accelerate\n",
      "[-6.6396695e-01  8.9712339e-05]\n",
      "Action: Don't accelerate\n",
      "[-0.6628553   0.00111163]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6597294   0.00312594]\n",
      "Action: Accelerate to the Left\n",
      "[-0.6566106   0.00311878]\n",
      "Action: Accelerate to the Right\n",
      "[-0.6515205  0.0050901]\n",
      "Action: Accelerate to the Right\n",
      "[-0.64449435  0.00702614]\n",
      "Action: Accelerate to the Right\n",
      "[-0.63558125  0.00891311]\n",
      "Action: Accelerate to the Right\n",
      "[-0.624844    0.01073727]\n",
      "Action: Accelerate to the Left\n",
      "[-0.614359    0.01048498]\n",
      "Action: Accelerate to the Left\n",
      "[-0.60420173  0.01015731]\n",
      "Action: Accelerate to the Left\n",
      "[-0.59444577  0.00975595]\n",
      "Action: Don't accelerate\n",
      "[-0.5841625   0.01028331]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5724274   0.01173505]\n",
      "Action: Accelerate to the Right\n",
      "[-0.5593274   0.01309995]\n",
      "Action: Accelerate to the Left\n",
      "[-0.54696006  0.01236741]\n",
      "Action: Accelerate to the Left\n",
      "[-0.53541756  0.01154247]\n",
      "Action: Don't accelerate\n",
      "[-0.5237865   0.01163109]\n",
      "Action: Don't accelerate\n",
      "[-0.512154   0.0116325]\n",
      "Action: Don't accelerate\n",
      "[-0.5006073   0.01154668]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4902329   0.01037438]\n",
      "Action: Don't accelerate\n",
      "[-0.48010835  0.01012456]\n",
      "Action: Don't accelerate\n",
      "[-0.47030905  0.0097993 ]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4599077   0.01040133]\n",
      "Action: Don't accelerate\n",
      "[-0.44998118  0.00992655]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4396023  0.0103789]\n",
      "Action: Accelerate to the Right\n",
      "[-0.42884672  0.01075557]\n",
      "Action: Accelerate to the Left\n",
      "[-0.41979226  0.00905446]\n",
      "Action: Accelerate to the Right\n",
      "[-0.4105038   0.00928843]\n",
      "Action: Accelerate to the Left\n",
      "[-0.4030474  0.0074564]\n",
      "Action: Accelerate to the Left\n",
      "[-0.39747557  0.00557185]\n",
      "Action: Accelerate to the Left\n",
      "[-0.39382723  0.00364833]\n",
      "Action: Don't accelerate\n",
      "[-0.3911278   0.00269945]\n",
      "Action: Don't accelerate\n",
      "[-0.38939592  0.00173186]\n",
      "Action: Accelerate to the Right\n",
      "[-0.38764364  0.00175231]\n",
      "Action: Accelerate to the Right\n",
      "[-0.38588294  0.00176068]\n",
      "Action: Don't accelerate\n",
      "[-0.385126    0.00075695]\n",
      "Action: Accelerate to the Right\n",
      "[-0.384378    0.00074802]\n",
      "Action: Accelerate to the Left\n",
      "[-0.38564402 -0.00126605]\n",
      "Action: Don't accelerate\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'observation_episode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-c301dc61187a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Action: Accelerate to the Right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#ejecución de la acción elegida\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mobservation_episode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#cerramos la visualización del entorno\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'observation_episode' is not defined"
     ]
    }
   ],
   "source": [
    "for i_episode in range(1000):\n",
    "    observation = env.reset()\n",
    "    for t in range(200):\n",
    "        print(observation)\n",
    "        action = env.action_space.sample() #acción aleatoria\n",
    "        if action == 0:\n",
    "            print('Action: Accelerate to the Left')\n",
    "        elif action == 1:\n",
    "            print(\"Action: Don't accelerate\")\n",
    "        else:print('Action: Accelerate to the Right')\n",
    "        observation, reward, done, info = env.step(action) #ejecución de la acción elegida\n",
    "    observation_episode.append(observation)\n",
    "env.close() #cerramos la visualización del entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-ls7BUfzJXO"
   },
   "source": [
    "## 2. Agente DQN (2.5 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY-1yE_rzJXO"
   },
   "source": [
    "En este apartado implementaremos una DQN teniendo en cuenta la exploración-explotación (epsilon-*greedy*), la red objetivo, y el buffer de repetición de experiencias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGMSF2_DzJXO"
   },
   "source": [
    "Definiremos el buffer como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nOVGNzjqyNyL"
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cajqLdkWyNyN"
   },
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer', \n",
    "            field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, \n",
    "                                   replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        return batch\n",
    "\n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(\n",
    "            self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-Kdu1XBzJXP"
   },
   "source": [
    "Primeramente implementaremos la red neuronal, utilizando un modelo Secuencial con la siguiente configuración:\n",
    "<ul>\n",
    "    <li>Tres capas completamente conectadas (representadas en pytorch por <code>nn.Lineal</code>) con 512, 216 y 128 neuronas cada una, <code>bias=True</code>, y activación ReLU </li>\n",
    "    <li>Una capa de salida completamente conectada y <code>bias=True</code> </li>\n",
    "</ul>\n",
    "\n",
    "Usaremos el optimizador Adam para entrenar la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkZkUxCuzJXP"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio (0.75 ptos):</strong> Implementar la clase <code>NeuralNet()</code>. Inicializar las variables necesarias y definir el modelo Secuencial de red neuronal indicado.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "V_zF7yu3zJXP"
   },
   "outputs": [],
   "source": [
    "class NeuralNet(torch.nn.Module):  \n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        \n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estadps\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        actions: array de acciones posibles\n",
    "        \"\"\"\n",
    "        ######################################\n",
    "        ##TODO: Inicializar parámetros\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        \n",
    "       \n",
    "        #######################################\n",
    "        ##TODO: Neural network\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 512, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128, bias=True),\n",
    "            torch.nn.Linear(128, self.n_outputs, bias=True))\n",
    "        \n",
    "        \n",
    "        #######################################\n",
    "        ##TODO: Initialize optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "                                          \n",
    "                                          \n",
    "    ### e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.choice(self.actions)  # acción random\n",
    "        else:\n",
    "            qvals = self.get_qvals(state)  # acción del cálculo de Q para esta acción\n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        state_t = torch.FloatTensor(state)\n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53o_VHkBzJXQ"
   },
   "source": [
    "A continuación implementaremos una clase que defina el comportamiento del agente DQN teniendo en cuenta:\n",
    "    <ul>\n",
    "        <li>La exploración/explotación (decaimiento de epsilon)</li>\n",
    "        <li>La actualización y sincronización de la red principal y la red objetivo (pérdida)</li>\n",
    "    </ul>\n",
    "    \n",
    "    \n",
    "Consideraremos que el agente ha aprendido a realizar la tarea (i.e. el \"juego\" termina) cuando obtiene una media de mínimo -110 puntos durante 100 episodios consecutivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YotfiaNDzJXQ"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (75 ptos):</strong> Implementar los siguientes puntos de la clase <code>DQNAgent()</code>:\n",
    "    <ol>\n",
    "        <li>Declarar las variables de la clase</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Actualizar la red principal según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la pérdida (ecuación Bellman, etc)</li> \n",
    "        <li>Sincronizar la red objetivo según la frecuencia establecida en los hiperparámetros</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "        <li>Comprobar límite de episodios</li>\n",
    "        <li>Actualizar epsilon según: $$\\textrm{max}(\\epsilon · \\epsilon_{\\textrm{decay}}, 0.01)$$ </li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias cada 100 episodios</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "        <li>La posición final del coche en cada episodio</li>\n",
    "        <li>La evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "pg5NY6uLzJXQ"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HRqWgeW0zJXQ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    ###################################### DONE\n",
    "    ##TODO 1: Declarar variables\n",
    "    def __init__(self, env, main_network, buffer, reward_threshold, epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        \n",
    "        self.env = env\n",
    "        self.main_network = main_network\n",
    "        self.target_network = deepcopy(main_network) \n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = epsilon\n",
    "        self.batch_size = batch_size\n",
    "        self.reward_threshold = reward_threshold\n",
    "        self.nblock = 100 # bloque de los X últimos episodios de los que se calculará la media de recompensas\n",
    "        self.initialize()           \n",
    "   \n",
    "    ###################################### DONE\n",
    "    ##TODO 2: Inicializar otras variables necesarias\n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "    ###################################### DONE\n",
    "    ##TODO 3: Tomar paso siguiente\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore': \n",
    "            action = self.env.action_space.sample()  # acción aleatoria en el burn-in\n",
    "        else:\n",
    "            action = self.main_network.get_action(self.state0, eps) # acción a partir del valor de Q (elección de la acción con mejor Q)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        #TODO: tomar 'step' i obtener nuevo estado y recompensa. Guardar la experiencia en el buffer DONE\n",
    "        new_state, reward, done, _ = self.env.step(action)\n",
    "        self.total_reward += reward\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) # guardamos experiencia en el buffer\n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #TODO: resetear entorno 'if done' DONE\n",
    "        if done:\n",
    "            self.state0 = env.reset()\n",
    "        return done\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    ## TRAINING\n",
    "    def train(self, gamma=0.99, max_episodes=5000, \n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=3,\n",
    "              dnn_sync_frequency=1000):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Rellenamos el buffer con N experiencias aleatorias ()\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "            \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # El agente toma una acción\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "                \n",
    "                #################################################################################\n",
    "                #####TODO 4:  Actualizar la red principal según la frecuencia establecida #######\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                ########################################################################################\n",
    "                ###TODO 6: Sincronizar red principal y red objetivo según la frecuencia establecida#####\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.main_network.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                \n",
    "                \n",
    "   \n",
    "                \n",
    "                \n",
    "                if gamedone:                   \n",
    "                    episode += 1\n",
    "                    self.training_rewards.append(self.total_reward) # guardamos las recompensas obtenidas\n",
    "                    self.update_loss = []   \n",
    "                \n",
    "                    \n",
    "                    #######################################################################################\n",
    "                    ###TODO 7: calcular la media de recompensa de los últimos X episodios, y almacenar#####\n",
    "                    mean_rewards = np.mean(\n",
    "                        self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(\n",
    "                        episode, mean_rewards, self.epsilon), end=\"\")\n",
    "                    \n",
    "                    #######################################################################################\n",
    "                    ### TODO 8: Comprobar que todavía quedan episodios. Parar el aprendizaje si se llega al límite\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "\n",
    "                    \n",
    "                    #Termina el juego si la media de recompensas ha llegado al umbral fijado para este juego  \n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(\n",
    "                            episode))\n",
    "                        break\n",
    "                    \n",
    "                    \n",
    "                    #################################################################################\n",
    "                    ######TODO 9: Actualizar epsilon según la velocidad de decaimiento fijada########\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "              \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    #####################################         \n",
    "    #####TODO 5: cálculo de la pérdida###\n",
    "    def calculate_loss(self, batch):\n",
    "        # Separamos las variables de la experiencia y las convertimos a tensores \n",
    "        states, actions, rewards, dones, next_states = [i for i in batch] \n",
    "        rewards_vals = torch.FloatTensor(rewards)\n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1)\n",
    "        dones_t = torch.ByteTensor(dones)\n",
    "        \n",
    "        # Obtenemos los valores de Q de la red principal\n",
    "        qvals = torch.gather(self.main_network.get_qvals(states), 1, actions_vals)\n",
    "        # Obtenemos los valores de Q objetivo. El parámetro detach() evita que estos valores actualicen la red objetivo\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states),\n",
    "                               dim=-1)[0].detach()\n",
    "        qvals_next[dones_t] = 0 # 0 en estados terminales\n",
    "        \n",
    "        #################################################################################\n",
    "        ### TODO: Calcular ecuación de Bellman\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "        \n",
    "        #################################################################################\n",
    "        ### TODO: Calcular la pérdida (MSE)\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def update(self):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) # seleccionamos un conjunto del buffer\n",
    "        loss = self.calculate_loss(batch) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9aacIO4yNyV"
   },
   "source": [
    "A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.001</li>\n",
    "        <li>Tamaño del batch: 32</li>\n",
    "        <li>Número de episodios: 5000</li>\n",
    "        <li>Número de episodios para rellenar el buffer: 1000</li>\n",
    "        <li>Frecuencia de actualización de la red neuronal: 3 </li>\n",
    "        <li>Frecuencia de sincronización con la red objetivo: 1000</li>\n",
    "        <li>Capacidad máxima del buffer: 10000</li>\n",
    "        <li>Factor de descuento: 0.99</li>\n",
    "        <li>Epsilon: 1, con decaimiento de 0.99</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtWA3A9WyNyV"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Declarar los hiperparámetros, cargar el modelo de red neuronal y entrenar el agente. Guardar el modelo entrenado en formato \".pth\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001            #Velocidad de aprendizaje\n",
    "BATCH_SIZE = 32       #Conjunto a coger del buffer para la red neuronal\n",
    "MAX_EPISODES = 5000   #Número máximo de episodios (el agente debe aprender antes de llegar a este valor)\n",
    "MEMORY_SIZE = 1000    #Máxima capacidad del buffer\n",
    "DNN_UPD = 3           #Frecuencia de actualización de la red neuronal \n",
    "DNN_SYNC = 1000       #Frecuencia de sincronización de pesos entre la red neuronal y la red objetivo\n",
    "GAMMA = 0.99          #Valor gamma de la ecuación de Bellman\n",
    "EPSILON = 1           #Valor inicial de epsilon\n",
    "EPSILON_DECAY = .99   #Decaimiento de epsilon\n",
    "BURN_IN = 1000        #Número de episodios iniciales usados para rellenar el buffer antes de entrenar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling replay buffer...\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:160: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorAdvancedIndexing.cpp:1273.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 827 Mean Rewards -200.00 Epsilon 1\t\t"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-6c8ee902ea25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmain_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward_threshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPSILON_DECAY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, \n\u001b[1;32m----> 5\u001b[1;33m               batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-06838515e074>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, gamma, max_episodes, batch_size, dnn_update_frequency, dnn_sync_frequency)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m#####TODO 4:  Actualizar la red principal según la frecuencia establecida #######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_count\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdnn_update_frequency\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-06838515e074>\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalculate_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# calculamos la pérdida\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# hacemos la diferencia para obtener los gradientes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# aplicamos los gradientes a la red neuronal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;31m# Guardamos los valores de pérdida\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m                    eps=group['eps'])\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "buffer = experienceReplayBuffer(memory_size=MEMORY_SIZE, burn_in=BURN_IN)\n",
    "main_network = NeuralNet(env, learning_rate=lr)\n",
    "agent = DQNAgent(env, main_network, buffer, reward_threshold = env.spec.reward_threshold, epsilon=EPSILON, eps_decay=EPSILON_DECAY, batch_size=BATCH_SIZE)\n",
    "agent.train(gamma=GAMMA, max_episodes=MAX_EPISODES, \n",
    "              batch_size=BATCH_SIZE, dnn_update_frequency=DNN_UPD, dnn_sync_frequency=DNN_SYNC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_vYhUcsyNyh"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gráfico con las recompensas obtenidas a lo largo del entrenamieno, la evolución de las recompensas medias cada 100 episodios, y el umbral de recompensa establecido por el entorno.</li>\n",
    "        <li>Gráfico con la posición final del coche en cada episodio</li>\n",
    "        <li>Gráfico con la evolución de la perdida a lo largo del entrenamiento</li>\n",
    "        <li>Gráfico con la evolución de epsilon a lo largo del entrenamiento</li>\n",
    "    </ol>\n",
    "\n",
    "Comentar los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'agent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6fda9c9308d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_rewards\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'agent' is not defined"
     ]
    }
   ],
   "source": [
    "agent.training_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(agent.training_rewards, label='Rewards')\n",
    "plt.plot(agent.mean_training_rewards, label='Mean Rewards')\n",
    "plt.axhline(agent.reward_threshold, color='r', label=\"Reward threshold\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Rewards')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvQTGX9-C2r8"
   },
   "source": [
    "Una vez entrenado el agente, nos interesa comprobar cómo de bien ha aprendido y si es capaz de conseguir que el coche llegue a su objetivo. Para ello, recuperamos el modelo entrenado y dejamos que el agente tome acciones aleatorias según ese modelo y observamos su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "859cm-vKyNy5"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.5 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 100 episodios consecutivos. Almacenar la posición final del agente en cada episodio, y calcular la suma de recompensas por cada episodio. Mostrar:\n",
    "    <ul>\n",
    "        <li>un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido por el entorno</li>\n",
    "        <li> un gráfico con la posición final del coche en cada episodio</li>\n",
    "    </ul>\n",
    "\n",
    "Comentar los resultados obtenidos.\n",
    "\n",
    "---------------------------------------------------------------------------------------------------\n",
    "<i>Opcional</i>: visualizar los episodios con <code>env.render()</code>, sólo posible en local. Esta visualización ralentiza el proceso unos segundos por episodio.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zu8QjJ_LyNyG"
   },
   "source": [
    "## 3. Agente DDQN (2.5 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtTQvNPJyNyG"
   },
   "source": [
    "En este apartado implementaremos una doble DQN con la misma red neuronal y características de la DQN del apartado anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GsW7n9AeyNyP"
   },
   "source": [
    "Recordemos que con la doble DQN, es la red principal la que elije la acción con mayor valor de Q, y la red objetivo la que proporciona el valor objetivo de Q para esa\n",
    "acción, es decir, la que dirá qué recompensa tiene esa acción elegida por la red principal. \n",
    "\n",
    "Siguiendo el mismo esquema que en el ejercicio anterior, a continuación implementaremos una clase que defina el entrenamiento del agente teniendo en cuenta: \n",
    "    <ul>\n",
    "        <li>La exploración/explotación (decaimiento de epsilon)</li>\n",
    "        <li>La actualización y sincronización de la red principal y la red objetivo (pérdida)</li>\n",
    "    </ul>\n",
    "    \n",
    "Consideraremos que el agente ha aprendido a realizar la tarea (i.e. el \"juego\" termina) cuando obtiene una media de mínimo -110 puntos durante 100 episodios consecutivos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_-cpMzgyNyQ"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (1 pto):</strong> Implementar los siguientes puntos de la clase <code>DDQNAgent()</code> siguiendo el mismo esquema de implementación realizado con el método <code>DQNAgent()</code>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "geNcnkj5yNyS"
   },
   "outputs": [],
   "source": [
    "class DDQNAgent:\n",
    "\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UgNdP0Q2zJXY"
   },
   "source": [
    "A continuación entrenaremos el modelo con los mismos hiperparámetros que con la DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HEET99Fg8V-"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Entrenar el agente. Guardar el modelo entrenado en formato \".pth\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qMZj-rLzJXZ"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Representar:\n",
    "    <ol>\n",
    "        <li>Gráfico con las recompensas obtenidas a lo largo del entrenamieno, la evolución de las recompensas medias cada 100 episodios, y el umbral de recompensa establecido por el entorno</li>\n",
    "        <li>Gráfico con la posición final del coche en cada episodio</li>\n",
    "        <li>Gráfico con la evolución de la perdida a lo largo del entrenamiento</li>\n",
    "        <li>Gráfico con la evolución de epsilon a lo largo del entrenamiento</li>\n",
    "  </ol>\n",
    "Comentar los resultados obtenidos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGI4zhoZzJXa"
   },
   "source": [
    "Una vez entrenado el agente, nos interesa comprobar cómo de bien ha aprendido, si el \"robot\" es capaz de realizar las tareas aprendidas. Para ello, recuperamos el modelo entrenado y dejamos que el agente tome acciones aleatorias según ese modelo y observamos su comportamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYbtLYtFzJXa"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.5 ptos):</strong> Cargar el modelo entrenado y ejecutar el agente entrenado durante 100 episodios consecutivos. Almacenar la posición final del agente en cada episodio, y calcular la suma de recompensas por cada episodio. Mostrar:\n",
    "    <ul>\n",
    "        <li>un gráfico con la suma de las recompensas respecto de los episodios, incluyendo el umbral de recompensa establecido por el entorno</li>\n",
    "        <li> un gráfico con la posición final del coche en cada episodio</li>\n",
    "    </ul>\n",
    "\n",
    "Comentar los resultados obtenidos.\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------\n",
    "<i>Opcional</i>: visualizar los episodios con <code>env.render()</code>, sólo posible en local. Esta visualización ralentiza el proceso unos segundos por episodio.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TSeDMmIDyNzx"
   },
   "source": [
    "## 4. REINFORCE con línea de base (2 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pCX29gujLx0"
   },
   "source": [
    "En este apartado implementaremos un modelo basado en Gradientes de Política, el método de REINFORCE con línea de base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjoLM_eiyNzx"
   },
   "source": [
    "Utilizaremos un modelo Secuencial con la siguiente configuración:\n",
    "<ul>\n",
    "    <li>Tres capas completamente conectadas de 64, 32 y 16 neuronas respectivamente, <code>bias=True</code>, y activación tangencial</li>\n",
    "    <li>Una capa de salida completamente conectada, <code>bias=True</code>, y activación Softmax (dim=-1)\n",
    "</ul>\n",
    "\n",
    "Usaremos el optimizador Adam para entrenar la red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOm-7BMDyNzx"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.75 ptos):</strong> Implementar la clase <code>PGReinforce()</code>. Inicializar las variables necesarias y definir el modelo Secuencial de red neuronal indicado.\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u9uCoGrQyNzy"
   },
   "outputs": [],
   "source": [
    "class NeuralNetReinforce(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, env, learning_rate=1e-3):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        n_inputs: tamaño del espacio de estadps\n",
    "        n_outputs: tamaño del espacio de acciones\n",
    "        learning_rate: velocidad aprendizaje\n",
    "        \"\"\"       \n",
    "        super(NeuralNetReinforce, self).__init__()\n",
    "        \n",
    "        ###################################\n",
    "        ####TODO: Inicializar parámetros#### \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        #######################################\n",
    "        ##TODO: Construcción de la red neuronal \n",
    "        self.model = None\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #######################################\n",
    "        ##TODO: Inicializar el optimizador\n",
    "        self.optimizer = None\n",
    "        \n",
    "\n",
    "\n",
    "    #Obtención de las probabilidades de las posibles acciones       \n",
    "    def get_action_prob(self, state):\n",
    "        action_probs = self.model(torch.FloatTensor(state))\n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4MkqfqUuzJXc"
   },
   "source": [
    "A continuación definiremos el comportamiento del agente. En este caso no tendremos en cuenta el umbral de recompensa en la finalización del aprendizaje, sino **dejaremos al agente realizar el máximo de episodios establecido**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FusJMueNyNz0"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.75 ptos):</strong> Implementar la clase <code>reinforceAgent()</code> teniendo en cuenta los siguientes puntos:\n",
    "    <ol>\n",
    "        <li>Inicializar parámetros</li>\n",
    "        <li>Inicializar las variables necesarias</li>\n",
    "        <li>Implementar la acción a tomar</li>\n",
    "        <li>Calcular el <i>discounted rewards</i> usando como línea de base la media del retorno estandarizada</li>\n",
    "        <li>Calcular la media de recompensas de los últimos 100 episodios</li>\n",
    "        <li>Actualizar red, almacenar pérdida y resetear variables cuando se completa el batch</li>\n",
    "        <li>Implementar la pérdida por actualización</li>\n",
    "        <li>Hacer las comprobaciones necesarias de fin de episodio</li>\n",
    "    </ol>\n",
    "Además, durante el proceso se deben almacenar (*):\n",
    "    <ul>\n",
    "        <li>Las recompensas obtenidas en cada paso del entrenamiento</li>\n",
    "        <li>Las recompensas medias cada 100 episodios</li>\n",
    "        <li>La posición final del coche en cada episodio</li>\n",
    "        <li>La pérdida durante el entrenamiento</li>\n",
    "    </ul>\n",
    "\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------------\n",
    "<b>Nota</b>: se os proporciona el código pre-implementado. La implementación que se pide en el enunciado está indicada en los bloques <i>TODO</i> y/o con variables igualadas a <i>None</i>, salvo (*) en qué momento almacenar las variables que se indican.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "Z9gBfz0UyNz0"
   },
   "outputs": [],
   "source": [
    "class reinforceAgent:\n",
    "    \n",
    "    def __init__(self, env, main_network):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ======\n",
    "        env: entorno\n",
    "        main_network: clase con la red neuronal diseñada\n",
    "        nblock: bloque de los X últimos episodios de los que se calculará la media de recompensa\n",
    "        \"\"\"    \n",
    "        \n",
    "        ###################################################\n",
    "        ######TODO 1: inicializar parámetros ##################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.initialize()\n",
    "\n",
    "        \n",
    "    ###############################################################\n",
    "    #####TODO 2: inicializar variables extra que se necesiten######:\n",
    "    def initialize(self):\n",
    "        self.batch_rewards = []\n",
    "        self.batch_actions = []\n",
    "        self.batch_states = []\n",
    "        self.batch_counter = 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    ## TRAINING\n",
    "    def train(self, gamma=0.99, max_episodes=2000, batch_size=10):\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        episode = 0\n",
    "        action_space = np.arange(self.env.action_space.n)\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            state0 = env.reset()\n",
    "            episode_states = [] #array para almacenar estados\n",
    "            episode_rewards = [] #array para almacenar rewards\n",
    "            episode_actions = [] #array para almacenar acciones\n",
    "            gamedone = False\n",
    "            \n",
    "            while gamedone == False:\n",
    "                ###########################################################\n",
    "                ######TODO 3:  Tomar nueva acción ##############################################\n",
    "                action_probs = None  #distribución de probabilidad de las acciones dado el estado actual\n",
    "                action = None   #acción aleatoria de la distribución de probabilidad\n",
    "                next_state, reward, gamedone, _ = None\n",
    "                \n",
    "                                \n",
    "                #TODO: Almacenamos experiencias (estados, rewards y acciones) que se van obteniendo en este episodio\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                state0 = None\n",
    "                \n",
    "                \n",
    "                if gamedone:\n",
    "                    episode += 1\n",
    "                    \n",
    "                    # Calculamos el término del retorno menos la línea de base\n",
    "                    self.batch_rewards.extend(self.discount_rewards(episode_rewards))\n",
    "                    self.batch_states.extend(episode_states)\n",
    "                    self.batch_actions.extend(episode_actions)\n",
    "                    \n",
    "                    \n",
    "                    #####################################################################################\n",
    "                    ###TODO 5: calcular media de recompensas de los últimos X episodios#####\n",
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    #####################################################################################\n",
    "                    ###TODO 6: Cuando se completa el tamaño del batch: actualizar la red (update), almacenar \n",
    "                    #la pérdida del entrenamiento, y resetear las variables del episodio necesarias\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                  \n",
    "                    self.batch_counter += 1\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f}\\t\\t\".format(\n",
    "                        episode, mean_rewards), end=\"\")\n",
    "         \n",
    "        \n",
    "                    #####################################################################################\n",
    "                    ###TODO 8:Hacer las comprobaciones necesarias de fin de episodio\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                          \n",
    "\n",
    "    ########################################################\n",
    "    ###TODO 4: cálculo del retorno menos la línea de base###                  \n",
    "    def discount_rewards(self, rewards):\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    " \n",
    "\n",
    "    #################################################\n",
    "    ###TODO 7: Cálculo de la pérdida#################\n",
    "    # Recordatorio: cada actualización es proporcional al producto del retorno y el gradiente de la probabilidad\n",
    "    # de tomar la acción tomada, dividido por la probabilidad de tomar esa acción (logaritmo natural)\n",
    "    def calculate_loss(self, state_t, action_t, reward_t):\n",
    "        logprob = torch.log(self.main_network.get_action_prob(state_t))\n",
    "        selected_logprobs = None\n",
    "        loss = None\n",
    "        return loss\n",
    "\n",
    "\n",
    "        \n",
    "    ## Actualización                \n",
    "    def update(self, batch_s, batch_r, batch_a):\n",
    "        self.main_network.optimizer.zero_grad()  # eliminamos cualquier gradiente pasado\n",
    "        state_t = torch.FloatTensor(batch_s)\n",
    "        reward_t = torch.FloatTensor(batch_r)       \n",
    "        action_t = torch.LongTensor(batch_a)             \n",
    "        loss = self.calculate_loss(state_t, action_t, reward_t) # calculamos la pérdida\n",
    "        loss.backward() # hacemos la diferencia para obtener los gradientes\n",
    "        self.main_network.optimizer.step() # aplicamos los gradientes a la red neuronal\n",
    "        # Guardamos los valores de pérdida\n",
    "        self.update_loss.append(loss.detach().numpy())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BAefENuyNz2"
   },
   "source": [
    "### Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8qru0f53yNz2"
   },
   "source": [
    "A continuación entrenaremos el modelo con los siguientes hiperparámetros:\n",
    "   <ul>\n",
    "        <li>Velocidad de aprendizaje: 0.01</li>\n",
    "        <li>Tamaño del batch: 8</li>\n",
    "        <li>Gamma: 0.99</li>\n",
    "        <li>Número máximo de episodios: 5000</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lWBA9D-JyNz2"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Definir los hiperparámetros, cargar el modelo de red neuronal y entrenar el agente\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK35p3r4yNz9"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Análisis (0.25 ptos):</strong> ¿Qué ha ocurrido? ¿El agente es capaz de aprender la tarea? ¿Por qué? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFWK0jmpzJXe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wfNFwFuzJXe"
   },
   "source": [
    "### 4.1 Modificación de la recompensa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5N4iYJtzJXf"
   },
   "source": [
    "Como hemos visto, el entorno de MountainCar únicamente proporciona recompensa cuando el agente llega al objetivo. Pero no hay otras indicaciones que permitan al agente saber que está yendo en la buena dirección de aprendizaje, lo que hace el aprendizaje un poco aleatorio y difícil para el agente. \n",
    "\n",
    "En este apartado vamos a modificar la recompensa del agente REINFORCE creando una función que dé al agente una recompensa según la posición en la que se encuentra. Que aunque no llegue al objetivo final pueda al menos saber que no iba tan mal encaminado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ-TgotlzJXf"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.25 ptos):</strong> Definir una función <code>get_reward()</code> en la que, si el agente se encuentra:\n",
    "<ul>\n",
    "<li>en una posición mayor o igual a 0.5, la recompensa será: +10</li>\n",
    "<li>en una posición mayor que -0.4, la recompensa será: $(1+s)^2$ , es decir, proporcional a la posición</li>\n",
    "<li>en cualquier otra posición, se le penalizará con una recompensa de: -1 </li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7cC8D9TzJXf"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (0.5 ptos):</strong> Modificar la clase <code>reinforceAgent</code> del ejercicio anterior para que el agente reciba las nuevas recompensas. Entrenar el agente con los mismos parámetros que en el ejercicio anterior, y mostrar las gráficas de recompensas y de la posición final del coche en cada episodio.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "eurDaz_11OMq"
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zxs9sT0llza7"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Análisis (0.2 ptos):</strong> Comentar los resultados obtenidos. ¿Ha conseguido el agente REINFORCE aprender a llegar al objetivo? Indicar qué se podría hacer para mejorar el resultado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQvAa-AvyN0L"
   },
   "source": [
    "## 5. Comparación de modelos (0.5 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxF6XdywyN0L"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio:</strong> Mostrar en un mismo gráfico la evolución de la media de recompensas de los modelos DQN y DDQN, junto con el umbral de recompensa. Comentar los resultados.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhIZshE7Xawz"
   },
   "source": [
    "## 6. Creación de un entorno en Gym (2 ptos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_1J3ZQQ2zJXh"
   },
   "source": [
    "En este ejercicio diseñaremos un entorno sencillo siguiendo el esquema de los entornos de <code>gym</code>, y trataremos de resolverlo.\n",
    "\n",
    "Los entornos de <code>gym</code> suelen tener la siguiente estructura:\n",
    "\n",
    "```\n",
    "class FooEnv(gym.Env):\n",
    "  metadata = {'render.modes': ['human']}\n",
    "\n",
    "  def __init__(self):\n",
    "    ...\n",
    "  def step(self, action):\n",
    "    ...\n",
    "  def reset(self):\n",
    "    ...\n",
    "  def render(self, mode='human', close=False):\n",
    "    ...\n",
    " \n",
    " ```\n",
    " \n",
    "Para nuestro entorno usaremos todas estas funciones salvo el <code>render</code> (relativo al diseño de la visualización el entorno)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7U1O0HWGzJXh"
   },
   "source": [
    "Primeramente importamos la librerías necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "yuHxDz3ZXawz"
   },
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kl1cEczLzJXh"
   },
   "source": [
    "### 6.1 Entorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_icId-jzJXi"
   },
   "source": [
    "El entorno que vamos a diseñar consiste en un regulador de temperatura. Imaginemos que queremos tener una ducha inteligente que sea capaz de mantener la temperatura en un intervalo concreto (37-39 grados) durante todo el tiempo de baño que fijemos. Cada acción será subir o bajar la temperatura un grado, o mantener la temperatura actual.\n",
    "\n",
    "Concretamente queremos que el regulador de temperatura tenga las siguientes características:\n",
    "    <ul>\n",
    "        <li>La temperatura (i.e. estado) puede oscilar entre los 0 y los 100 grados</li>\n",
    "        <li>La temperatura inicial (i.e. estado inicial) será de 38+-3 grados</li>\n",
    "        <li>La duración del baño debe ser de 3 minutos (180 segundos)</li>\n",
    "        <li>Las acciones posibles seran:\n",
    "            <ul>\n",
    "                <li>0 : bajar temperatura un grado</li>\n",
    "                <li>1 : mantener temperatura</li>\n",
    "                <li>2 : subir temperatura un grado</li>\n",
    "            </ul></li>\n",
    "        <li>La recompensa será de +1 si la temperatura se encuentra entre 37 y 39 grados, de lo contrario será -1</li>\n",
    "        <li>El episodio termina cuando el tiempo de baño se agota (se cumplen los 180 segundos)</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLrYAbIBzJXi"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio (0.75 ptos):</strong> Diseñar el entorno <code>TempControlEnv</code> con las características indicadas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "fe9EFgu9zJXi"
   },
   "outputs": [],
   "source": [
    "class TempControlEnv(Env):\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Argumentos:\n",
    "            - espacio de acciones\n",
    "            - espacio de observaciones (se recomienda usar la función Box)\n",
    "            - estado inicial\n",
    "            - duración episodio\n",
    "        '''\n",
    "        ###TODO: inicializar variables###\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def step(self, action):\n",
    "        '''\n",
    "        Argumentos:\n",
    "            - temperatura según acción tomada\n",
    "            - disminución tiempo de baño\n",
    "        '''\n",
    "     \n",
    "        ###TODO: nueva temperatura y còmput de reducción del tiempo de baño###\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "        \n",
    "        ###TODO: Cálculo de las recompensas###\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        ###TODO: Comprobar si el tiempo de baño se ha agotado###\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        # Aplicamos ruido a la temperatura (exe: alguien abre el agua caliente de otro grifo\n",
    "        # y altera la temperatura del agua de nuestra ducha)\n",
    "        self.state += random.randint(-1,1)  # Apply some temperature noise \n",
    "        \n",
    "        info = {} # Set placeholder for info\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    \n",
    "    \n",
    "    def reset(self):  \n",
    "        '''\n",
    "        Argumentos:\n",
    "            - resetear temperatura inicial\n",
    "            - resetear tiempo de baño\n",
    "        '''\n",
    "        \n",
    "        ###TODO: resetear variables###\n",
    "\n",
    "    \n",
    "    \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmlDKL_WzJXi"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "    <strong>Ejercicio (0.25 ptos):</strong> Cargar el entorno <code>TempControlEnv</code> y mostrar el espacio de acciones y el espacio de observaciones. Ejecutar 10 episodios con acciones aleatorias, mostrando el episodio y la recompensa obtenida.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_ZXX9atzJXj"
   },
   "source": [
    "### 6.2 Agente regulador de la temperatura"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T36bmAaYzJXj"
   },
   "source": [
    "<div style=\"background-color: #EDF7FF; border-color: #7C9DBF; border-left: 5px solid #7C9DBF; padding: 0.5em;\">\n",
    "<strong>Ejercicio (1 pto):</strong> Implementar un agente capaz de regular la temperatura y mantenerla en los intervalos establecidos por el entorno <code>TempControlEnv</code> diseñado. Se puede implementar cualquier tipo de agente visto en el curso (módulos 9, 10 y 11) con los hiperparámetros que se considere conveniente.\n",
    "    <ul>\n",
    "        <li>Mostrar gráficamente la evolución de las recompensas</li>\n",
    "        <li>Mostrar gráficamente la evolución de la temperatura final de cada episodio a lo largo del entrenamiento</li>\n",
    "        <li>Testear el modelo de agente con 100 episodios y mostrar las gráficas de recompensas y temperatura</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "M2_883_RL_PEC2_Solucion_v4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
